from typing import Annotated, Sequence, Any, Literal, Dict
from langchain_core.messages import HumanMessage, AIMessage
from pathlib import Path
import logging
import json
import asyncio
import sys
import os

from .state import AgentState
from src.tools.rag.rag import retrieve_documents, calculate_retrieval_quality
from src.tools.rag.entity_linking import link_entities
from llm.router import ModelRouter

# FastMCP å®¢æˆ·ç«¯ç›¸å…³å¯¼å…¥
from fastmcp import FastMCP

logger = logging.getLogger(__name__)

# åˆå§‹åŒ– ModelRouterï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_router = None

def get_router() -> ModelRouter:
    """è·å– ModelRouter å•ä¾‹"""
    global _router
    if _router is None:
        # config.yaml åœ¨ FGO-agent/llm/config.yaml
        project_root = Path(__file__).parent.parent.parent
        config_path = project_root / "llm" / "config.yaml"
        _router = ModelRouter(str(config_path))
    return _router

# ============================================================================
# èŠ‚ç‚¹å®šä¹‰ï¼ˆä»…å®šä¹‰æ¥å£ï¼Œä¸åšå…·ä½“å®ç°ï¼‰
# ============================================================================

async def query_classify_node(state: AgentState) -> Dict[str, Any]:
    """
    æŸ¥è¯¢åˆ†ç±»èŠ‚ç‚¹ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥åˆ¤æ–­æŸ¥è¯¢ç±»å‹ã€‚
    
    åŠŸèƒ½ï¼š
    1. å®ä½“é“¾æ¥ï¼šå°†åˆ«åæ›¿æ¢ä¸ºæ ‡å‡†å…¨åï¼ˆè§„åˆ™ï¼‰
    2. ä¸Šä¸‹æ–‡æŒ‡ä»£æ¶ˆè§£ï¼šå°†ä»£è¯æ›¿æ¢ä¸ºå…·ä½“å®ä½“ï¼ˆLLMï¼‰
    3. æŸ¥è¯¢ä¼˜åŒ–ï¼šæ ¹æ®å¤±è´¥åŸå› æ”¹å†™æŸ¥è¯¢ï¼ˆLLMï¼‰
    4. æŸ¥è¯¢åˆ†ç±»ï¼šåˆ¤æ–­æŸ¥è¯¢ç±»å‹ï¼ˆknowledge_base/web_search/endï¼‰
    
    Returns:
        æ›´æ–° query_classification, original_query, rewritten_query
    """
    logger.info("=== è¿›å…¥æŸ¥è¯¢åˆ†ç±»èŠ‚ç‚¹ ===")
    
    # 1. æå–ç”¨æˆ·æŸ¥è¯¢
    messages = state.get("messages", [])
    user_query = None
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    if not user_query:
        logger.warning("æœªæ‰¾åˆ°ç”¨æˆ·æŸ¥è¯¢ï¼Œé»˜è®¤è¿”å› end")
        return {
            "query_classification": "end",
            "original_query": "",
        }
    
    logger.info(f"ç”¨æˆ·åŸå§‹æŸ¥è¯¢: '{user_query}'")
    
    # 2. æŸ¥è¯¢æ”¹å†™æµç¨‹
    current_query = user_query
    
    # è·å–ä¸Šä¸€æ¬¡è¯„ä¼°çš„å¤±è´¥ä¿¡æ¯
    retry_count = state.get("retry_count", 0) or 0
    evaluation_reason = state.get("evaluation_reason", "")
    retrieval_score = state.get("retrieval_score", 0.0)
    
    if retry_count > 0:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°é‡è¯•ï¼ˆç¬¬ {retry_count} æ¬¡ï¼‰")
        logger.info(f"ğŸ“Š ä¸Šæ¬¡æ£€ç´¢è´¨é‡åˆ†æ•°: {retrieval_score:.3f}")
        logger.info(f"ğŸ“ å¤±è´¥åŸå› : {evaluation_reason}")
    
    # ===== æ­¥éª¤2.1: å®ä½“é“¾æ¥ï¼ˆè§„åˆ™æ˜ å°„ï¼‰ =====
    linked_query = link_entities(current_query)
    if linked_query != current_query:
        logger.info(f"ğŸ”— å®ä½“é“¾æ¥: '{current_query}' â†’ '{linked_query}'")
        current_query = linked_query
    
    # ===== æ­¥éª¤2.2: ä¸Šä¸‹æ–‡æŒ‡ä»£æ¶ˆè§£ + æŸ¥è¯¢ä¼˜åŒ–ï¼ˆLLMï¼‰ =====
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨ LLM æ”¹å†™
    need_rewrite = False
    rewrite_reason = []
    
    # æ¡ä»¶1: æœ‰é‡è¯•ï¼ˆè¯´æ˜ä¸Šæ¬¡æ£€ç´¢å¤±è´¥ï¼‰
    if retry_count > 0:
        need_rewrite = True
        rewrite_reason.append("æ£€ç´¢å¤±è´¥é‡è¯•")
    
    # æ¡ä»¶2: æŸ¥è¯¢ä¸­åŒ…å«æŒ‡ä»£è¯
    pronouns = ["å¥¹", "ä»–", "å®ƒ", "è¿™ä¸ª", "é‚£ä¸ª", "è¿™", "é‚£", "å‰è€…", "åè€…"]
    if any(pronoun in current_query for pronoun in pronouns):
        need_rewrite = True
        rewrite_reason.append("åŒ…å«æŒ‡ä»£è¯")
    
    if need_rewrite:
        logger.info(f"âœï¸ éœ€è¦ LLM æ”¹å†™æŸ¥è¯¢ï¼ŒåŸå› : {', '.join(rewrite_reason)}")
        
        # æ„å»ºæ”¹å†™ Promptï¼ˆç›´æ¥å†™åœ¨èŠ‚ç‚¹ä¸­ï¼‰
        rewrite_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŸ¥è¯¢æ”¹å†™ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¼˜åŒ–ç”¨æˆ·çš„æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨ FGO ä»è€…çŸ¥è¯†åº“ä¸­æ£€ç´¢ã€‚

**æ”¹å†™è§„åˆ™**ï¼š
1. **æŒ‡ä»£æ¶ˆè§£**ï¼šå°†ä»£è¯ï¼ˆå¥¹/ä»–/è¿™ä¸ª/é‚£ä¸ªç­‰ï¼‰æ›¿æ¢ä¸ºå…·ä½“çš„ä»è€…åç§°
2. **æŸ¥è¯¢ä¼˜åŒ–**ï¼šä½¿æŸ¥è¯¢æ›´æ¸…æ™°ã€å…·ä½“ï¼Œä¾¿äºæ£€ç´¢
3. **ä¿ç•™æ„å›¾**ï¼šä¸æ”¹å˜ç”¨æˆ·çš„åŸå§‹æ„å›¾
4. **ä¿æŒç®€æ´**ï¼šä¸æ·»åŠ å¤šä½™ä¿¡æ¯

**é‡è¦**ï¼š
- åªè¿”å›æ”¹å†™åçš„æŸ¥è¯¢ï¼Œä¸è¦ä»»ä½•è§£é‡Š
- å¦‚æœæ— æ³•æ”¹å†™æˆ–ä¸éœ€è¦æ”¹å†™ï¼Œè¿”å›åŸæŸ¥è¯¢
- ç¡®ä¿æ”¹å†™åçš„æŸ¥è¯¢æ˜¯å®Œæ•´çš„ã€å¯ç‹¬ç«‹ç†è§£çš„

ç¤ºä¾‹1:
å†å²: ç”¨æˆ·é—®"ç›ä¿®çš„å®å…·æ˜¯ä»€ä¹ˆ"ï¼ŒAIå›ç­”"..."
å½“å‰æŸ¥è¯¢: "å¥¹çš„æŠ€èƒ½å‘¢"
æ”¹å†™: "ç›ä¿®çš„æŠ€èƒ½æ˜¯ä»€ä¹ˆ"

ç¤ºä¾‹2:
å†å²: ç”¨æˆ·é—®"é˜¿å°”æ‰˜è‰é›…å‰å®³å—"
å½“å‰æŸ¥è¯¢: "å¥¹çš„å®å…·æ•ˆæœ"
æ”¹å†™: "é˜¿å°”æ‰˜è‰é›…çš„å®å…·æ•ˆæœ"""

        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘3è½®ï¼‰
        history_context = []
        for msg in messages[-6:]:  # æœ€è¿‘3è½®ï¼ˆ3ä¸ªç”¨æˆ· + 3ä¸ªAIï¼‰
            if isinstance(msg, HumanMessage):
                history_context.append(f"ç”¨æˆ·: {msg.content}")
            elif isinstance(msg, AIMessage):
                # æˆªå– AI å›å¤çš„å‰100å­—ï¼ˆé¿å…å¤ªé•¿ï¼‰
                content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                history_context.append(f"AI: {content}")
        
        history_text = "\n".join(history_context) if history_context else "æ— å†å²å¯¹è¯"
        
        # æ„å»ºæ”¹å†™è¯·æ±‚
        rewrite_user_prompt = f"""**å†å²å¯¹è¯**:
{history_text}

**å½“å‰æŸ¥è¯¢**: {current_query}"""
        
        # å¦‚æœæœ‰å¤±è´¥åŸå› ï¼Œæ·»åŠ åˆ° prompt
        if evaluation_reason:
            rewrite_user_prompt += f"\n\n**ä¸Šæ¬¡æ£€ç´¢å¤±è´¥åŸå› **: {evaluation_reason}"
            rewrite_user_prompt += "\n\nè¯·æ ¹æ®å¤±è´¥åŸå› ä¼˜åŒ–æŸ¥è¯¢ï¼Œä½¿å…¶æ›´å®¹æ˜“æ£€ç´¢åˆ°æ­£ç¡®ç»“æœã€‚"
        
        rewrite_messages = [
            {"role": "system", "content": rewrite_system_prompt},
            {"role": "user", "content": rewrite_user_prompt}
        ]
        
        try:
            router = get_router()
            logger.info("è°ƒç”¨ LLM è¿›è¡ŒæŸ¥è¯¢æ”¹å†™")
            
            result, instance_name, physical_model_name, failover_events = await router.chat(
                messages=rewrite_messages,
                model="fgo-chat-model",
                stream=False
            )
            
            rewritten = result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            
            if rewritten and rewritten != current_query:
                logger.info(f"âœ¨ LLM æ”¹å†™: '{current_query}' â†’ '{rewritten}'")
                current_query = rewritten
            else:
                logger.info("â„¹ï¸ LLM æœªè¿›è¡Œæœ‰æ•ˆæ”¹å†™ï¼Œä¿æŒåŸæŸ¥è¯¢")
        
        except Exception as e:
            logger.warning(f"âš ï¸ LLM æ”¹å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæŸ¥è¯¢")
    
    # æœ€ç»ˆæ”¹å†™ç»“æœ
    rewritten_query = current_query
    if rewritten_query != user_query:
        logger.info(f"ğŸ“Œ æœ€ç»ˆæ”¹å†™ç»“æœ: '{user_query}' â†’ '{rewritten_query}'")
    
    # 3. æŸ¥è¯¢åˆ†ç±»
    router = get_router()
    
    # æ„é€ åˆ†ç±» prompt
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢åˆ†ç±»åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ¤æ–­ç”¨æˆ·æŸ¥è¯¢åº”è¯¥ä½¿ç”¨å“ªç§æ–¹å¼å¤„ç†ã€‚

**çŸ¥è¯†åº“æŸ¥è¯¢ï¼ˆknowledge_baseï¼‰**ï¼š
- FGO ä»è€…çš„åŸºç¡€èµ„æ–™ï¼ˆèŒé˜¶ã€æ˜Ÿçº§ã€å±æ€§ã€CVç­‰ï¼‰
- FGO ä»è€…çš„æŠ€èƒ½ä¿¡æ¯ï¼ˆæŠ€èƒ½åç§°ã€æ•ˆæœã€å†·å´æ—¶é—´ç­‰ï¼‰
- FGO ä»è€…çš„å®å…·ä¿¡æ¯ï¼ˆå®å…·åç§°ã€ç±»å‹ã€æ•ˆæœç­‰ï¼‰
- FGO ä»è€…çš„è§’è‰²èµ„æ–™å’ŒèƒŒæ™¯æ•…äº‹
- FGO ä»è€…çš„ç´ æéœ€æ±‚ï¼ˆçµåŸºå†ä¸´ã€æŠ€èƒ½å¼ºåŒ–æ‰€éœ€ç´ æï¼‰

**ç½‘ç»œæœç´¢ï¼ˆweb_searchï¼‰**ï¼š
- å®æ—¶ä¿¡æ¯æŸ¥è¯¢ï¼ˆæ´»åŠ¨æ—¶é—´ã€å¡æ± ä¿¡æ¯ã€ç‰ˆæœ¬æ›´æ–°ç­‰ï¼‰
- æ”»ç•¥å’Œç©æ³•å»ºè®®ï¼ˆé˜Ÿä¼é…ç½®ã€å…³å¡æ”»ç•¥ç­‰ï¼‰
- ç¤¾åŒºè®¨è®ºå’Œç©å®¶å¿ƒå¾—

**é—²èŠç»“æŸï¼ˆendï¼‰**ï¼š
- é—®å€™ã€é—²èŠ
- é FGO ç›¸å…³é—®é¢˜

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªç§æ–¹å¼å¤„ç†ã€‚åªéœ€è¦è¿”å› JSON æ ¼å¼ï¼š
{"classification": "knowledge_base" | "web_search" | "end", "reason": "åˆ†ç±»ç†ç”±"}"""

    user_prompt = f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}"
    
    llm_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    try:
        logger.info("è°ƒç”¨ LLM è¿›è¡ŒæŸ¥è¯¢åˆ†ç±»")
        
        # è°ƒç”¨ LLMï¼ˆéæµå¼ï¼‰
        result, instance_name, physical_model_name, failover_events = await router.chat(
            messages=llm_messages,
            model="fgo-chat-model",
            stream=False
        )
        
        # è§£æ LLM å“åº”
        llm_response = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        logger.info(f"LLM åˆ†ç±»å“åº”: {llm_response}")
        
        # å°è¯•è§£æ JSON
        try:
            # æå– JSONï¼ˆå¯èƒ½è¢« markdown åŒ…è£¹ï¼‰
            if "```json" in llm_response:
                json_str = llm_response.split("```json")[1].split("```")[0].strip()
            elif "```" in llm_response:
                json_str = llm_response.split("```")[1].split("```")[0].strip()
            else:
                json_str = llm_response.strip()
            
            parsed = json.loads(json_str)
            classification = parsed.get("classification", "knowledge_base")
            reason = parsed.get("reason", "")
            
            logger.info(f"åˆ†ç±»ç»“æœ: {classification}, ç†ç”±: {reason}")
            
            # é¦–æ¬¡åˆ†ç±»æ—¶ retry_count = 0ï¼Œé‡è¯•æ—¶ä¿æŒä¸å˜
            result = {
                "query_classification": classification,
                "original_query": user_query,
                "rewritten_query": rewritten_query,
            }
            if retry_count == 0:
                result["retry_count"] = 0
            
            return result
            
        except json.JSONDecodeError:
            logger.warning("JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±» knowledge_base")
            result = {
                "query_classification": "knowledge_base",
                "original_query": user_query,
                "rewritten_query": rewritten_query,
            }
            if retry_count == 0:
                result["retry_count"] = 0
            return result
    
    except Exception as e:
        logger.error(f"æŸ¥è¯¢åˆ†ç±»å¤±è´¥: {str(e)}", exc_info=True)
        # é»˜è®¤ä½¿ç”¨çŸ¥è¯†åº“
        result = {
            "query_classification": "knowledge_base",
            "original_query": user_query,
            "rewritten_query": rewritten_query,
        }
        if retry_count == 0:
            result["retry_count"] = 0
        return result


def knowledge_base_node(state: AgentState) -> Dict[str, Any]:
    """
    çŸ¥è¯†åº“ RAG èŠ‚ç‚¹ï¼Œä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. ç¡®å®šæŸ¥è¯¢æ–‡æœ¬ï¼ˆä¼˜å…ˆä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢ï¼‰
    2. è°ƒç”¨ RAG æ£€ç´¢å™¨è¿›è¡Œå‘é‡æ£€ç´¢å’Œé‡æ’åº
    3. å°†æ£€ç´¢ç»“æœå­˜å…¥ state
    
    Returns:
        æ›´æ–° retrieved_docs
    """
    logger.info("=== è¿›å…¥çŸ¥è¯†åº“æ£€ç´¢èŠ‚ç‚¹ ===")
    
    # 1. ç¡®å®šæŸ¥è¯¢æ–‡æœ¬
    # ä¼˜å…ˆçº§ï¼šrewritten_query > original_query > ä» messages æå–
    query = None
    
    if state.get("rewritten_query"):
        query = state["rewritten_query"]
        logger.info(f"ä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢: {query}")
    elif state.get("original_query"):
        query = state["original_query"]
        logger.info(f"ä½¿ç”¨åŸå§‹æŸ¥è¯¢: {query}")
    else:
        # ä» messages ä¸­æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        messages = state.get("messages", [])
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                query = msg.content
                logger.info(f"ä»æ¶ˆæ¯å†å²æå–æŸ¥è¯¢: {query}")
                break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŸ¥è¯¢æ–‡æœ¬ï¼Œè¿”å›ç©ºç»“æœ
    if not query:
        logger.warning("æœªæ‰¾åˆ°æŸ¥è¯¢æ–‡æœ¬ï¼Œè¿”å›ç©ºç»“æœ")
        return {
            "retrieved_docs": [],
            "retrieval_score": 0.0
        }
    
    # 2. æ‰§è¡Œ RAG æ£€ç´¢ï¼ˆåŒ…å«å‘é‡æ£€ç´¢ + CrossEncoder é‡æ’åºï¼‰
    try:
        logger.info(f"å¼€å§‹æ£€ç´¢æ–‡æ¡£ï¼ŒæŸ¥è¯¢: '{query}'")
        documents = retrieve_documents(
            query=query,
            top_k=5,  # è¿”å› top 5 æ–‡æ¡£
            rerank=True,  # å¯ç”¨é‡æ’åº
            rerank_method="crossencoder"  # ä½¿ç”¨ CrossEncoder é‡æ’åº
        )
        
        logger.info(f"æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # è®°å½•æ£€ç´¢ç»“æœæ‘˜è¦
        if documents:
            for i, doc in enumerate(documents[:3], 1):  # åªæ‰“å°å‰3ä¸ª
                logger.info(
                    f"  æ–‡æ¡£{i}: {doc['metadata'].get('servant_name', 'N/A')} - "
                    f"{doc['metadata'].get('type', 'N/A')} "
                    f"(åˆ†æ•°: {doc.get('rerank_score', doc.get('score', 0)):.3f})"
                )
        
        return {
            "retrieved_docs": documents
        }
        
    except Exception as e:
        logger.error(f"æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
        # æ£€ç´¢å¤±è´¥æ—¶è¿”å›ç©ºç»“æœ
        return {
            "retrieved_docs": [],
            "retrieval_score": 0.0
        }


async def rag_evaluation_node(state: AgentState) -> Dict[str, Any]:
    """
    RAG è¯„ä¼°èŠ‚ç‚¹ï¼Œè¯„ä¼°æ£€ç´¢ç»“æœçš„è´¨é‡ã€‚
    
    è¯„ä¼°ç»“æœï¼š
    - "pass": æ£€ç´¢ç»“æœè‰¯å¥½ï¼Œè¿›å…¥ LLM ç”ŸæˆèŠ‚ç‚¹
    - "rewrite": æ£€ç´¢ç»“æœä¸ä½³ä¸”æœªè¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œæ”¹å†™æŸ¥è¯¢å›åˆ°åˆ†ç±»èŠ‚ç‚¹
    
    æ³¨æ„ï¼šå¦‚æœé‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œå³ä½¿è´¨é‡ä¸ä½³ä¹Ÿè¿”å› "pass"
    
    Returns:
        æ›´æ–° evaluation_result, retrieval_score, retry_count
    """
    logger.info("=== è¿›å…¥ RAG è¯„ä¼°èŠ‚ç‚¹ ===")
    
    # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
    MAX_RETRY = 2
    
    # 1. è·å–æ£€ç´¢ç»“æœå’Œå½“å‰é‡è¯•æ¬¡æ•°
    retrieved_docs = state.get("retrieved_docs", [])
    retry_count = state.get("retry_count", 0) or 0
    original_query = state.get("original_query", "")
    
    logger.info(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªæ–‡æ¡£ï¼Œå½“å‰é‡è¯•æ¬¡æ•°: {retry_count}")
    
    # 2. å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£
    if not retrieved_docs:
        logger.warning("æœªæ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£")
        
        # åˆ¤æ–­æ˜¯å¦å¯ä»¥é‡è¯•
        if retry_count < MAX_RETRY:
            logger.info(f"å°è¯•æ”¹å†™æŸ¥è¯¢é‡è¯•ï¼ˆ{retry_count + 1}/{MAX_RETRY}ï¼‰")
            return {
                "evaluation_result": "rewrite",
                "retrieval_score": 0.0,
                "retry_count": retry_count + 1,
                "evaluation_reason": "æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£ï¼Œå»ºè®®è¡¥å…¨ä»è€…å…¨åæˆ–æ˜ç¡®æŸ¥è¯¢çš„æ•°æ®ç±»å‹ï¼ˆæŠ€èƒ½/å®å…·/èµ„æ–™/ç´ æï¼‰"
            }
        else:
            logger.warning(f"å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRY}ï¼Œå¼ºåˆ¶é€šè¿‡")
            return {
                "evaluation_result": "pass",
                "retrieval_score": 0.0,
                "retry_count": retry_count
            }
    
    # 3. è®¡ç®—æ£€ç´¢è´¨é‡åˆ†æ•°
    quality_score = calculate_retrieval_quality(retrieved_docs)
    logger.info(f"æ£€ç´¢è´¨é‡åˆ†æ•°: {quality_score:.3f}")
    
    # 4. å‡†å¤‡æ–‡æ¡£æ‘˜è¦ç»™ LLM è¯„ä¼°
    doc_summaries = []
    for i, doc in enumerate(retrieved_docs[:3], 1):  # åªå±•ç¤ºå‰3ä¸ªæ–‡æ¡£
        servant_name = doc['metadata'].get('servant_name', 'N/A')
        doc_type = doc['metadata'].get('type', 'N/A')
        score = doc.get('rerank_score', doc.get('score', 0))
        content_preview = doc['content'][:100] + "..." if len(doc['content']) > 100 else doc['content']
        
        doc_summaries.append(
            f"æ–‡æ¡£{i}ï¼š{servant_name} - {doc_type}ï¼ˆåˆ†æ•°: {score:.3f}ï¼‰\nå†…å®¹é¢„è§ˆ: {content_preview}"
        )
    
    doc_summary_text = "\n\n".join(doc_summaries)
    
    # 5. è°ƒç”¨ LLM è¿›è¡Œè¯„ä¼°
    router = get_router()
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢è´¨é‡è¯„ä¼°åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦èƒ½å¤Ÿå›ç­”ç”¨æˆ·çš„æŸ¥è¯¢ã€‚

**è¯„ä¼°æ ‡å‡†**ï¼š
1. ç›¸å…³æ€§ï¼šæ–‡æ¡£å†…å®¹æ˜¯å¦ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³
2. å®Œæ•´æ€§ï¼šæ–‡æ¡£æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”æŸ¥è¯¢
3. å‡†ç¡®æ€§ï¼šæ–‡æ¡£æ¥æºæ˜¯å¦æ­£ç¡®ï¼ˆä»è€…åç§°ã€æ•°æ®ç±»å‹ç­‰ï¼‰

**è¯„ä¼°ç»“æœ**ï¼š
- "pass": æ–‡æ¡£è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ç”¨æ¥ç”Ÿæˆç­”æ¡ˆ
- "rewrite": æ–‡æ¡£è´¨é‡ä¸ä½³ï¼Œå»ºè®®æ”¹å†™æŸ¥è¯¢é‡æ–°æ£€ç´¢

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨è¿™äº›æ–‡æ¡£ã€‚åªéœ€è¦è¿”å› JSON æ ¼å¼ï¼š
{"result": "pass" | "rewrite", "reason": "è¯„ä¼°ç†ç”±"}"""

    user_prompt = f"""ç”¨æˆ·æŸ¥è¯¢ï¼š{original_query}

æ£€ç´¢è´¨é‡åˆ†æ•°ï¼š{quality_score:.3f}ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰

æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆå‰3ä¸ªï¼‰ï¼š
{doc_summary_text}

è¯·è¯„ä¼°è¿™äº›æ–‡æ¡£æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢ã€‚"""

    llm_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    try:
        logger.info("è°ƒç”¨ LLM è¿›è¡Œæ£€ç´¢è´¨é‡è¯„ä¼°")
        
        # è°ƒç”¨ LLMï¼ˆéæµå¼ï¼‰
        result, instance_name, physical_model_name, failover_events = await router.chat(
            messages=llm_messages,
            model="fgo-chat-model",
            stream=False
        )
        
        # è§£æ LLM å“åº”
        llm_response = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        logger.info(f"LLM è¯„ä¼°å“åº”: {llm_response}")
        
        # å°è¯•è§£æ JSON
        try:
            # æå– JSONï¼ˆå¯èƒ½è¢« markdown åŒ…è£¹ï¼‰
            if "```json" in llm_response:
                json_str = llm_response.split("```json")[1].split("```")[0].strip()
            elif "```" in llm_response:
                json_str = llm_response.split("```")[1].split("```")[0].strip()
            else:
                json_str = llm_response.strip()
            
            parsed = json.loads(json_str)
            llm_result = parsed.get("result", "pass")
            reason = parsed.get("reason", "")
            
            logger.info(f"LLM è¯„ä¼°ç»“æœ: {llm_result}, ç†ç”±: {reason}")
            
            # 6. æ ¹æ® LLM è¯„ä¼°ç»“æœå’Œé‡è¯•æ¬¡æ•°å†³å®š
            if llm_result == "rewrite" and retry_count < MAX_RETRY:
                logger.info(f"LLM å»ºè®®æ”¹å†™æŸ¥è¯¢ï¼Œå‡†å¤‡é‡è¯•ï¼ˆ{retry_count + 1}/{MAX_RETRY}ï¼‰")
                return {
                    "evaluation_result": "rewrite",
                    "retrieval_score": quality_score,
                    "retry_count": retry_count + 1,
                    "evaluation_reason": reason  # ğŸ‘ˆ æºå¸¦ LLM è¯„ä¼°çš„å¤±è´¥åŸå› 
                }
            else:
                # LLM å»ºè®® pass æˆ–å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°
                if llm_result == "rewrite" and retry_count >= MAX_RETRY:
                    logger.warning(f"LLM å»ºè®®æ”¹å†™ï¼Œä½†å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRY}ï¼Œå¼ºåˆ¶é€šè¿‡")
                else:
                    logger.info("LLM è¯„ä¼°é€šè¿‡ï¼Œè¿›å…¥ç”Ÿæˆé˜¶æ®µ")
                
                return {
                    "evaluation_result": "pass",
                    "retrieval_score": quality_score,
                    "retry_count": retry_count
                }
            
        except json.JSONDecodeError:
            logger.warning("JSON è§£æå¤±è´¥ï¼Œé»˜è®¤è¯„ä¼°ä¸º pass")
            return {
                "evaluation_result": "pass",
                "retrieval_score": quality_score,
                "retry_count": retry_count
            }
    
    except Exception as e:
        logger.error(f"LLM è¯„ä¼°å¤±è´¥: {str(e)}", exc_info=True)
        
        # LLM è¯„ä¼°å¤±è´¥ï¼Œå›é€€åˆ°åŸºäºè´¨é‡åˆ†æ•°çš„ç®€å•åˆ¤æ–­
        logger.info("å›é€€åˆ°åŸºäºè´¨é‡åˆ†æ•°çš„ç®€å•åˆ¤æ–­")
        
        # è´¨é‡åˆ†æ•°é˜ˆå€¼ï¼š> 0.6 ä¸ºåˆæ ¼
        if quality_score > 0.6:
            logger.info(f"è´¨é‡åˆ†æ•° {quality_score:.3f} > 0.6ï¼Œè¯„ä¼°é€šè¿‡")
            return {
                "evaluation_result": "pass",
                "retrieval_score": quality_score,
                "retry_count": retry_count
            }
        elif retry_count < MAX_RETRY:
            logger.info(f"è´¨é‡åˆ†æ•° {quality_score:.3f} <= 0.6ï¼Œå°è¯•æ”¹å†™ï¼ˆ{retry_count + 1}/{MAX_RETRY}ï¼‰")
            return {
                "evaluation_result": "rewrite",
                "retrieval_score": quality_score,
                "retry_count": retry_count + 1,
                "evaluation_reason": f"æ£€ç´¢è´¨é‡åˆ†æ•°è¾ƒä½ï¼ˆ{quality_score:.3f}ï¼‰ï¼Œæ–‡æ¡£ç›¸å…³æ€§ä¸è¶³ï¼Œå»ºè®®æ”¹å†™æŸ¥è¯¢"
            }
        else:
            logger.warning(f"è´¨é‡åˆ†æ•°ä½ä½†å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¼ºåˆ¶é€šè¿‡")
            return {
                "evaluation_result": "pass",
                "retrieval_score": quality_score,
                "retry_count": retry_count
            }


async def llm_generate_node(state: AgentState) -> Dict[str, Any]:
    """
    LLM ç”ŸæˆèŠ‚ç‚¹ï¼ŒåŸºäº RAG æ£€ç´¢çš„æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. è·å–ç”¨æˆ·æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£
    2. æ„é€  RAG promptï¼ˆæ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
    3. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    4. æ¸…ç†æ‰€æœ‰ä¸­é—´çŠ¶æ€ï¼Œåªä¿ç•™ messages
    
    Returns:
        æ›´æ–° messagesï¼ˆæ·»åŠ  AI å›å¤ï¼‰ï¼Œæ¸…ç†æ‰€æœ‰ä¸­é—´çŠ¶æ€å­—æ®µ
    """
    logger.info("=== è¿›å…¥ LLM ç”ŸæˆèŠ‚ç‚¹ ===")
    
    # 1. è·å–ç”¨æˆ·æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£
    messages = state.get("messages", [])
    user_query = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    retrieved_docs = state.get("retrieved_docs", [])
    
    if not user_query:
        logger.warning("æœªæ‰¾åˆ°ç”¨æˆ·æŸ¥è¯¢")
        return {
            "messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚")],
            # æ¸…ç†ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }
    
    if not retrieved_docs:
        logger.warning("æœªæ‰¾åˆ°æ£€ç´¢æ–‡æ¡£ï¼Œç”Ÿæˆå…œåº•ç­”æ¡ˆ")
        return {
            "messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°å…³äºã€Œ{user_query}ã€çš„ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸€ç§é—®æ³•æˆ–æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚")],
            # æ¸…ç†ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }
    
    logger.info(f"ç”¨æˆ·æŸ¥è¯¢: '{user_query}'")
    logger.info(f"åŸºäº {len(retrieved_docs)} ä¸ªæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ")
    
    # 2. æ„é€ æ–‡æ¡£ä¸Šä¸‹æ–‡
    doc_contexts = []
    for i, doc in enumerate(retrieved_docs, 1):
        servant_name = doc['metadata'].get('servant_name', 'N/A')
        doc_type = doc['metadata'].get('type', 'N/A')
        content = doc['content']
        
        doc_contexts.append(
            f"ã€å‚è€ƒèµ„æ–™ {i}ã€‘\n"
            f"æ¥æºï¼š{servant_name} - {doc_type}\n"
            f"å†…å®¹ï¼š\n{content}"
        )
    
    context_text = "\n\n".join(doc_contexts)
    
    # 3. æ„é€  RAG prompt
    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ FGOï¼ˆFate/Grand Orderï¼‰æ¸¸æˆåŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**å›ç­”è¦æ±‚**ï¼š
1. åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™è¿›è¡Œå›ç­”ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜
3. ç»„ç»‡è¯­è¨€æ¸…æ™°ã€æœ‰æ¡ç†ï¼Œä¾¿äºç†è§£
4. å¯ä»¥é€‚å½“è¡¥å……æ¸¸æˆç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†
5. ä½¿ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”

**å›ç­”æ ¼å¼**ï¼š
- ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸éœ€è¦è¯´"æ ¹æ®å‚è€ƒèµ„æ–™"ä¹‹ç±»çš„å‰ç¼€
- å¦‚æœæ˜¯åˆ—è¡¨ä¿¡æ¯ï¼ˆå¦‚æŠ€èƒ½ã€ç´ æï¼‰ï¼Œç”¨æ¸…æ™°çš„æ ¼å¼å±•ç¤º
- å¯ä»¥ç”¨è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§ï¼ˆå¦‚ â­ã€ğŸ¯ã€ğŸ’ ç­‰ï¼‰"""

    user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å‚è€ƒèµ„æ–™ï¼š
{context_text}

è¯·åŸºäºä»¥ä¸Šå‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""

    llm_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    # 4. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    router = get_router()
    
    try:
        logger.info("è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼ï¼‰")
        
        # è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
        stream_wrapper = await router.chat(
            messages=llm_messages,
            model="fgo-chat-model",
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # æ”¶é›†æµå¼å“åº”
        llm_response = ""
        async for chunk in stream_wrapper:
            if chunk.get("choices"):
                delta = chunk["choices"][0].get("delta", {})
                content = delta.get("content", "")
                if content:
                    llm_response += content
        
        # è·å–å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        if hasattr(stream_wrapper, '_metadata_dict'):
            instance_name = stream_wrapper._metadata_dict.get('instance_name')
            physical_model_name = stream_wrapper._metadata_dict.get('physical_model_name')
            logger.info(f"ä½¿ç”¨å®ä¾‹: {instance_name}, ç‰©ç†æ¨¡å‹: {physical_model_name}")
        
        if not llm_response:
            logger.warning("LLM è¿”å›ç©ºå“åº”")
            llm_response = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå…³äºã€Œ{user_query}ã€çš„ç­”æ¡ˆã€‚"
        
        logger.info(f"ç”Ÿæˆç­”æ¡ˆæˆåŠŸï¼Œé•¿åº¦: {len(llm_response)} å­—ç¬¦")
        
        # 5. è¿”å›ç»“æœå¹¶æ¸…ç†ä¸­é—´çŠ¶æ€
        return {
            "messages": [AIMessage(content=llm_response)],
            # æ¸…ç†æ‰€æœ‰ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }
    
    except Exception as e:
        logger.error(f"LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}", exc_info=True)
        
        # ç”Ÿæˆå¤±è´¥æ—¶ï¼Œè¿”å›å…œåº•ç­”æ¡ˆ
        fallback_answer = f"æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚ä¸è¿‡æˆ‘æ‰¾åˆ°äº†ä¸€äº›ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        
        # æå–å…³é”®ä¿¡æ¯ä½œä¸ºå…œåº•
        if retrieved_docs:
            first_doc = retrieved_docs[0]
            servant_name = first_doc['metadata'].get('servant_name', 'N/A')
            doc_type = first_doc['metadata'].get('type', 'N/A')
            content_preview = first_doc['content'][:200] + "..." if len(first_doc['content']) > 200 else first_doc['content']
            
            fallback_answer += f"æ¥æºï¼š{servant_name} - {doc_type}\n{content_preview}"
        else:
            fallback_answer = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”å…³äºã€Œ{user_query}ã€çš„é—®é¢˜ã€‚"
        
        return {
            "messages": [AIMessage(content=fallback_answer)],
            # æ¸…ç†ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }


async def web_search_node(state: AgentState) -> Dict[str, Any]:
    """
    ç½‘ç»œæœç´¢èŠ‚ç‚¹ï¼Œé€šè¿‡ FastMCP å®¢æˆ·ç«¯è°ƒç”¨ web_search MCP æœåŠ¡å™¨è¿›è¡Œæœç´¢å¹¶ç”Ÿæˆç­”æ¡ˆã€‚
    
    æµç¨‹ï¼š
    1. æå–ç”¨æˆ·æŸ¥è¯¢
    2. é€šè¿‡ FastMCP å®¢æˆ·ç«¯è°ƒç”¨ search_and_extract å·¥å…·
    3. è·å–ç½‘ç»œæœç´¢ç»“æœ
    4. LLM åŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    
    å…³é”®ï¼šæ­¤èŠ‚ç‚¹å†…éƒ¨å®Œæˆæœç´¢å’Œç­”æ¡ˆç”Ÿæˆï¼Œç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
    
    Returns:
        æ›´æ–° messagesï¼ˆæ·»åŠ  AI å›å¤ï¼‰ï¼Œæ¸…ç†ä¸­é—´çŠ¶æ€
    """
    logger.info("=== è¿›å…¥ç½‘ç»œæœç´¢èŠ‚ç‚¹ ===")
    
    # 1. è·å–ç”¨æˆ·æŸ¥è¯¢
    messages = state.get("messages", [])
    user_query = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    if not user_query:
        logger.warning("æœªæ‰¾åˆ°ç”¨æˆ·æŸ¥è¯¢")
        return {
            "messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚")],
            # æ¸…ç†ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }
    
    logger.info(f"ç”¨æˆ·æŸ¥è¯¢: '{user_query}'")
    
    # 2. è°ƒç”¨ FastMCP æœåŠ¡å™¨è¿›è¡Œç½‘ç»œæœç´¢
    async def call_fastmcp_search():
        """é€šè¿‡ FastMCP å®¢æˆ·ç«¯è°ƒç”¨ search_and_extract å·¥å…·"""
        # è·å– web_search.py çš„ç»å¯¹è·¯å¾„
        current_dir = Path(__file__).parent.parent
        web_search_script = current_dir / "tools" / "web_search" / "web_search.py"
        
        if not web_search_script.exists():
            logger.error(f"æœªæ‰¾åˆ° web_search.py: {web_search_script}")
            return None
        
        logger.info(f"è¿æ¥ FastMCP æœåŠ¡å™¨: {web_search_script}")
        
        try:
            # ä½¿ç”¨ FastMCP å®¢æˆ·ç«¯è¿æ¥åˆ°æœåŠ¡å™¨
            # FastMCP ä½¿ç”¨ stdio ä¼ è¾“ï¼Œé€šè¿‡å­è¿›ç¨‹å¯åŠ¨æœåŠ¡å™¨
            client = FastMCP("web-search-client")
            
            # é€šè¿‡ stdio è¿æ¥åˆ°æœåŠ¡å™¨
            async with client.stdio_client(
                command=sys.executable,
                args=[str(web_search_script)],
                env=None
            ) as connection:
                logger.info("FastMCP è¿æ¥å»ºç«‹æˆåŠŸ")
                
                # è°ƒç”¨ search_and_extract å·¥å…·
                logger.info(f"è°ƒç”¨å·¥å…·: search_and_extract, query={user_query}")
                
                result = await connection.call_tool(
                    "search_and_extract",
                    query=user_query,
                    max_results=5,
                    extract_count=3
                )
                
                # è§£æè¿”å›ç»“æœ
                if result:
                    logger.info(f"FastMCP æœç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
                    return result
                else:
                    logger.warning("FastMCP å·¥å…·è¿”å›ç©ºç»“æœ")
                    return None
        
        except Exception as e:
            logger.error(f"FastMCP è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            return None
    
    # æ‰§è¡Œæœç´¢
    try:
        search_results = await call_fastmcp_search()
    except Exception as e:
        logger.error(f"æ‰§è¡Œç½‘ç»œæœç´¢å¤±è´¥: {str(e)}", exc_info=True)
        search_results = None
    
    # 3. å¤„ç†æœç´¢ç»“æœ
    if not search_results:
        logger.warning("ç½‘ç»œæœç´¢å¤±è´¥æˆ–æœªæ‰¾åˆ°ç»“æœ")
        return {
            "messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨ç½‘ç»œä¸Šæ‰¾åˆ°å…³äºã€Œ{user_query}ã€çš„ç›¸å…³ä¿¡æ¯ã€‚")],
            # æ¸…ç†ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }
    
    logger.info(f"ç½‘ç»œæœç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(search_results)} å­—ç¬¦")
    
    # 4. è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ FGOï¼ˆFate/Grand Orderï¼‰æ¸¸æˆåŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®ç½‘ç»œæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**å›ç­”è¦æ±‚**ï¼š
1. åŸºäºæä¾›çš„ç½‘ç»œæœç´¢ç»“æœè¿›è¡Œå›ç­”
2. æ•´åˆå¤šä¸ªä¿¡æ¯æºï¼Œç»™å‡ºå…¨é¢ã€å‡†ç¡®çš„ç­”æ¡ˆ
3. å¦‚æœä¿¡æ¯ä¸ç¡®å®šï¼Œè¯·è¯šå®è¯´æ˜
4. ç»„ç»‡è¯­è¨€æ¸…æ™°ã€æœ‰æ¡ç†ï¼Œä¾¿äºç†è§£
5. å¯ä»¥å¼•ç”¨ä¿¡æ¯æ¥æºçš„é“¾æ¥
6. ä½¿ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”

**å›ç­”æ ¼å¼**ï¼š
- ç›´æ¥å›ç­”é—®é¢˜ï¼Œç®€æ´æ˜äº†
- å¿…è¦æ—¶å¯ä»¥åˆ†ç‚¹åˆ—ä¸¾
- å¯ä»¥ç”¨è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§ï¼ˆå¦‚ â­ã€ğŸ”ã€ğŸ“„ ç­‰ï¼‰"""

    user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

ç½‘ç»œæœç´¢ç»“æœï¼š
{search_results}

è¯·åŸºäºä»¥ä¸Šç½‘ç»œæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""

    llm_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    router = get_router()
    
    try:
        logger.info("è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ")
        
        # è°ƒç”¨ LLMï¼ˆéæµå¼ï¼‰
        result, instance_name, physical_model_name, failover_events = await router.chat(
            messages=llm_messages,
            model="fgo-chat-model",
            stream=False
        )
        
        # è§£æ LLM å“åº”
        llm_response = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        if not llm_response:
            logger.warning("LLM è¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨æœç´¢ç»“æœä½œä¸ºå…œåº•")
            llm_response = f"æ ¹æ®ç½‘ç»œæœç´¢ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹å…³äºã€Œ{user_query}ã€çš„ä¿¡æ¯ï¼š\n\n{search_results[:500]}..."
        
        logger.info(f"ç”Ÿæˆç­”æ¡ˆæˆåŠŸï¼Œé•¿åº¦: {len(llm_response)} å­—ç¬¦")
        
        # 5. è¿”å›ç»“æœå¹¶æ¸…ç†ä¸­é—´çŠ¶æ€
        return {
            "messages": [AIMessage(content=llm_response)],
            # æ¸…ç†æ‰€æœ‰ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }
    
    except Exception as e:
        logger.error(f"LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}", exc_info=True)
        
        # ç”Ÿæˆå¤±è´¥æ—¶ï¼Œç›´æ¥è¿”å›æœç´¢ç»“æœæ‘˜è¦
        fallback_answer = f"æ ¹æ®ç½‘ç»œæœç´¢ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹å…³äºã€Œ{user_query}ã€çš„ä¿¡æ¯ï¼š\n\n"
        fallback_answer += search_results[:800] + "..." if len(search_results) > 800 else search_results
        
        return {
            "messages": [AIMessage(content=fallback_answer)],
            # æ¸…ç†ä¸­é—´çŠ¶æ€
            "query_classification": None,
            "retry_count": None,
            "original_query": None,
            "rewritten_query": None,
            "retrieved_docs": None,
            "retrieval_score": None,
            "evaluation_result": None,
            "evaluation_reason": None,
        }

    
def end_node(state: AgentState) -> Dict[str, Any]:
    """
    ç»“æŸèŠ‚ç‚¹ï¼Œç›´æ¥ç»“æŸå¯¹è¯ï¼ˆä¾‹å¦‚é—²èŠã€é—®å€™ç­‰ï¼‰ã€‚
    
    Returns:
        æ›´æ–° messagesï¼Œæ¸…ç†ä¸­é—´çŠ¶æ€
    """
    return {
        "messages": [AIMessage(content="å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶è¯¢é—®ï¼")],
        # æ¸…ç†ä¸­é—´çŠ¶æ€
        "query_classification": None,
        "retry_count": None,
        "original_query": None,
    }
