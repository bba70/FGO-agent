"""
æ„å»º FGO å‘é‡æ•°æ®åº“

æµç¨‹ï¼š
1. è¯»å– textarea ä¸‹çš„æ‰€æœ‰ä»è€…æ•°æ®
2. ä½¿ç”¨ parse_wiki.py è§£æ
3. ä½¿ç”¨ FGOChunker åˆ‡åˆ† chunks
4. å­˜å…¥ ChromaDB
"""

import sys
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from data.parse_wiki import parse_full_wikitext
from src.tools.rag.chunker import FGOChunker
from database.kb.vectordb import get_vectordb


def load_all_servants(textarea_dir: Path) -> List[Dict[str, Any]]:
    """
    åŠ è½½æ‰€æœ‰ä»è€…æ•°æ®
    
    Args:
        textarea_dir: textarea ç›®å½•è·¯å¾„
    
    Returns:
        ä»è€…æ•°æ®åˆ—è¡¨ [{'name': '...', 'data': {...}}, ...]
    """
    print(f"\nğŸ“‚ è¯»å–ä»è€…æ•°æ®: {textarea_dir}")
    
    servants = []
    txt_files = list(textarea_dir.glob("*.txt"))
    
    if not txt_files:
        print(f"âŒ æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶")
        return servants
    
    print(f"âœ… æ‰¾åˆ° {len(txt_files)} ä¸ªä»è€…æ–‡ä»¶")
    
    for txt_file in tqdm(txt_files, desc="è§£æä»è€…æ•°æ®"):
        try:
            servant_name = txt_file.stem
            
            with open(txt_file, 'r', encoding='utf-8') as f:
                wikitext = f.read()
            
            parsed_data = parse_full_wikitext(wikitext)
            
            if parsed_data:
                servants.append({
                    'name': servant_name,
                    'data': parsed_data
                })
        
        except Exception as e:
            print(f"âš ï¸  è§£æ {txt_file.name} å¤±è´¥: {e}")
            continue
    
    print(f"âœ… æˆåŠŸè§£æ {len(servants)} ä¸ªä»è€…")
    return servants


def chunk_servants(servants: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    åˆ‡åˆ†ä»è€…æ•°æ®
    
    Args:
        servants: ä»è€…æ•°æ®åˆ—è¡¨
    
    Returns:
        chunks åˆ—è¡¨
    """
    print(f"\nâœ‚ï¸  åˆ‡åˆ†æ•°æ®å—...")
    chunker = FGOChunker()
    all_chunks = []
    
    for servant in tqdm(servants, desc="åˆ‡åˆ†æ•°æ®"):
        chunks = chunker.chunk_servant(servant['name'], servant['data'])
        all_chunks.extend(chunks)
    
    print(f"âœ… å…±åˆ‡åˆ†å‡º {len(all_chunks)} ä¸ªæ•°æ®å—")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = chunker.get_stats(all_chunks)
    print(f"\nğŸ“Š åˆ‡å—ç»Ÿè®¡ï¼š")
    print(f"   æ€»å—æ•°: {stats['total_chunks']}")
    print(f"   ä»è€…æ•°: {stats['total_servants']}")
    print(f"   å¹³å‡é•¿åº¦: {stats['avg_chunk_length']} å­—ç¬¦")
    print(f"   æŒ‰ç±»å‹åˆ†å¸ƒ:")
    for type_name, count in stats['chunks_by_type'].items():
        print(f"     {type_name}: {count} ä¸ª")
    
    # æ˜¾ç¤ºæ ·ä¾‹
    if all_chunks:
        print(f"\nğŸ“„ æ•°æ®å—ç¤ºä¾‹ï¼š")
        sample = all_chunks[0]
        print(f"   ID: {sample['id']}")
        print(f"   å…ƒæ•°æ®: {sample['metadata']}")
        print(f"   å†…å®¹é¢„è§ˆ: {sample['content'][:150]}...")
    
    return all_chunks


def insert_to_vectordb(chunks: List[Dict[str, Any]]):
    """
    æ’å…¥æ•°æ®åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        chunks: æ–‡æ¡£å—åˆ—è¡¨
    """
    print(f"\nğŸ“¦ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
    vectordb = get_vectordb()
    collection = vectordb.get_or_create_collection()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    existing_count = collection.count()
    if existing_count > 0:
        print(f"âš ï¸  é›†åˆä¸­å·²æœ‰ {existing_count} æ¡æ•°æ®")
        response = input("æ˜¯å¦æ¸…ç©ºç°æœ‰æ•°æ®ï¼Ÿ(y/n): ")
        if response.lower() == 'y':
            print("ğŸ—‘ï¸  æ¸…ç©ºç°æœ‰æ•°æ®...")
            vectordb.delete_collection(collection.name)
            collection = vectordb.get_or_create_collection()
        else:
            print("âš ï¸  ä¿ç•™ç°æœ‰æ•°æ®ï¼Œæ–°æ•°æ®å°†è¿½åŠ ")
    
    # æ‰¹é‡æ’å…¥
    print(f"\nğŸ“ æ’å…¥æ•°æ®åˆ°å‘é‡æ•°æ®åº“...")
    batch_size = 10  # æ¯æ‰¹æ¬¡æ’å…¥ 10 æ¡ï¼ˆQwen Embedding API é™åˆ¶ï¼‰
    
    for i in tqdm(range(0, len(chunks), batch_size), desc="æ’å…¥æ•°æ®"):
        batch = chunks[i:i+batch_size]
        
        try:
            collection.add(
                documents=[chunk['content'] for chunk in batch],
                metadatas=[chunk['metadata'] for chunk in batch],
                ids=[chunk['id'] for chunk in batch]
            )
        except Exception as e:
            print(f"\nâŒ æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±è´¥: {e}")
            # ç»§ç»­ä¸‹ä¸€æ‰¹
            continue
    
    print(f"âœ… æ•°æ®æ’å…¥å®Œæˆï¼")
    print(f"   æ€»æ–‡æ¡£æ•°: {collection.count()}")
    
    return collection


def test_retrieval(collection):
    """
    æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    
    Args:
        collection: ChromaDB é›†åˆ
    """
    print(f"\nğŸ” æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    
    test_queries = [
        "é˜¿å°”æ‰˜è‰é›…çš„åŸºç¡€æ•°å€¼",
        "æ¢…æ—çš„æŠ€èƒ½",
        "å‰å°”ä¼½ç¾ä»€çš„å®å…·",
        "èµ«æ‹‰å…‹å‹’æ–¯çš„ç´ æéœ€æ±‚"
    ]
    
    for query in test_queries:
        print(f"\n   æŸ¥è¯¢: {query}")
        try:
            results = collection.query(
                query_texts=[query],
                n_results=3
            )
            
            if results and results['ids'] and results['ids'][0]:
                for j, (doc_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0]), 1):
                    # ChromaDB è¿”å›è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
                    similarity = 1 / (1 + distance)
                    print(f"     {j}. ID: {doc_id}, ç›¸ä¼¼åº¦: {similarity:.3f}")
            else:
                print("     æœªæ‰¾åˆ°ç»“æœ")
        except Exception as e:
            print(f"     âŒ æŸ¥è¯¢å¤±è´¥: {e}")


def build_vectorstore():
    """ä¸»å‡½æ•°ï¼šæ„å»ºå‘é‡æ•°æ®åº“"""
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹æ„å»º FGO å‘é‡æ•°æ®åº“")
    print("=" * 80)
    
    try:
        # 1. åŠ è½½ä»è€…æ•°æ®
        textarea_dir = project_root / "data" / "textarea"
        servants = load_all_servants(textarea_dir)
        
        if not servants:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä»è€…æ•°æ®!")
            return
        
        # 2. åˆ‡åˆ†æ•°æ®
        chunks = chunk_servants(servants)
        
        if not chunks:
            print("âŒ æ²¡æœ‰åˆ‡åˆ†å‡ºä»»ä½•æ•°æ®å—!")
            return
        
        # 3. å­˜å…¥å‘é‡æ•°æ®åº“
        collection = insert_to_vectordb(chunks)
        
        # 4. æµ‹è¯•æ£€ç´¢
        test_retrieval(collection)
        
        # 5. å®Œæˆ
        vectordb = get_vectordb()
        print("\n" + "=" * 80)
        print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
        print("=" * 80)
        print(f"   å­˜å‚¨è·¯å¾„: {vectordb.config.persist_directory}")
        print(f"   é›†åˆåç§°: {vectordb.config.collection_name}")
        print(f"   æ–‡æ¡£æ€»æ•°: {collection.count()}")
        print(f"   Embeddingæ¨¡å‹: {vectordb.config.embedding_model_name}")
        print("=" * 80)
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\n\nâŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    build_vectorstore()

