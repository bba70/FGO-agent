# === ç½‘ç»œæœç´¢ä¿¡æ¯æå–ä¸å¤§æ¨¡å‹é›†æˆæ–¹æ¡ˆ ===

"""
å®Œæ•´çš„ç½‘ç»œæœç´¢ä¿¡æ¯æå–æµç¨‹ï¼š
1. æœç´¢ç»“æœè·å–
2. ç½‘é¡µå†…å®¹æå–
3. å†…å®¹æ¸…ç†å’Œç»“æ„åŒ–
4. é•¿åº¦æ§åˆ¶å’Œä¼˜åŒ–
5. å¤§æ¨¡å‹é›†æˆç­–ç•¥
"""

import asyncio
import httpx
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse, quote
from dataclasses import dataclass
from datetime import datetime
import hashlib

# æ ¸å¿ƒä¾èµ–åº“
from bs4 import BeautifulSoup
from readability import Document
import tiktoken
from newspaper import Article
import trafilatura

# ===== ç¬¬ä¸€éƒ¨åˆ†ï¼šæœç´¢ç»“æœè·å– =====

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    url: str
    snippet: str
    source: str
    rank: int = 0
    timestamp: datetime = None

class WebSearchEngine:
    """ç½‘ç»œæœç´¢å¼•æ“å°è£…"""
    
    def __init__(self):
        self.http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(30.0),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            },
            follow_redirects=True
        )
    
    async def search_duckduckgo(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """ä½¿ç”¨DuckDuckGoæœç´¢"""
        try:
            # DuckDuckGo HTMLæœç´¢ï¼ˆæ›´å¯é çš„æ–¹æ³•ï¼‰
            search_url = "https://html.duckduckgo.com/html/"
            params = {
                'q': query,
                'b': '',  # èµ·å§‹ä½ç½®
                'kl': 'cn-zh',  # ä¸­æ–‡åœ°åŒº
                's': '0'  # æ’åºæ–¹å¼
            }
            
            response = await self.http_client.post(search_url, data=params)
            response.raise_for_status()
            
            return self._parse_duckduckgo_html(response.text, max_results)
        
        except Exception as e:
            logging.error(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_duckduckgo_html(self, html: str, max_results: int) -> List[SearchResult]:
        """è§£æDuckDuckGo HTMLç»“æœ"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
        result_containers = soup.find_all('div', class_='result')
        
        for i, container in enumerate(result_containers[:max_results]):
            try:
                # æå–æ ‡é¢˜å’Œé“¾æ¥
                title_link = container.find('a', class_='result__a')
                if not title_link:
                    continue
                
                title = title_link.get_text(strip=True)
                url = title_link.get('href', '')
                
                # æå–æè¿°
                snippet_elem = container.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                if title and url:
                    results.append(SearchResult(
                        title=title,
                        url=url,
                        snippet=snippet,
                        source="duckduckgo",
                        rank=i + 1,
                        timestamp=datetime.now()
                    ))
            
            except Exception as e:
                logging.warning(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                continue
        
        return results
    
    async def search_with_google_api(self, query: str, api_key: str, cx: str, max_results: int = 10) -> List[SearchResult]:
        """ä½¿ç”¨Google Custom Search API"""
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': api_key,
                'cx': cx,
                'q': query,
                'num': min(max_results, 10),  # Google APIæœ€å¤š10ä¸ªç»“æœ
                'hl': 'zh-CN',
                'gl': 'cn'
            }
            
            response = await self.http_client.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for i, item in enumerate(data.get('items', [])):
                results.append(SearchResult(
                    title=item.get('title', ''),
                    url=item.get('link', ''),
                    snippet=item.get('snippet', ''),
                    source="google_api",
                    rank=i + 1,
                    timestamp=datetime.now()
                ))
            
            return results
        
        except Exception as e:
            logging.error(f"Googleæœç´¢å¤±è´¥: {e}")
            return []

# ===== ç¬¬äºŒéƒ¨åˆ†ï¼šç½‘é¡µå†…å®¹æå– =====

@dataclass
class ExtractedContent:
    """æå–çš„ç½‘é¡µå†…å®¹"""
    url: str
    title: str
    content: str
    summary: str
    metadata: Dict[str, Any]
    extraction_method: str
    token_count: int
    quality_score: float

class WebContentExtractor:
    """ç½‘é¡µå†…å®¹æå–å™¨"""
    
    def __init__(self, token_limit: int = 4000):
        self.token_limit = token_limit
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(20.0),
            headers={
                'User-Agent': 'Mozilla/5.0 (compatible; ContentBot/1.0)',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate'
            }
        )
    
    async def extract_multiple_urls(self, urls: List[str], max_concurrent: int = 3) -> List[ExtractedContent]:
        """å¹¶å‘æå–å¤šä¸ªURLçš„å†…å®¹"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def extract_with_semaphore(url):
            async with semaphore:
                return await self.extract_content(url)
        
        tasks = [extract_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_results = []
        for result in results:
            if isinstance(result, ExtractedContent):
                valid_results.append(result)
            elif isinstance(result, Exception):
                logging.warning(f"å†…å®¹æå–å¤±è´¥: {result}")
        
        return valid_results
    
    async def extract_content(self, url: str) -> Optional[ExtractedContent]:
        """æå–å•ä¸ªURLçš„å†…å®¹ï¼ˆå¤šæ–¹æ³•å°è¯•ï¼‰"""
        methods = [
            ("trafilatura", self._extract_with_trafilatura),
            ("newspaper", self._extract_with_newspaper),
            ("readability", self._extract_with_readability),
            ("beautifulsoup", self._extract_with_bs4)
        ]
        
        for method_name, method_func in methods:
            try:
                content = await method_func(url)
                if content and content.quality_score > 0.3:  # è´¨é‡é˜ˆå€¼
                    content.extraction_method = method_name
                    return content
            except Exception as e:
                logging.debug(f"æ–¹æ³• {method_name} æå–å¤±è´¥ {url}: {e}")
                continue
        
        logging.warning(f"æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥äº†: {url}")
        return None
    
    async def _extract_with_trafilatura(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨trafilaturaæå–ï¼ˆæ¨èæ–¹æ³•ï¼‰"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        # ä½¿ç”¨trafilaturaæå–
        content = trafilatura.extract(
            response.text,
            include_comments=False,
            include_tables=True,
            include_formatting=False
        )
        
        if not content:
            return None
        
        # æå–å…ƒæ•°æ®
        metadata = trafilatura.extract_metadata(response.text)
        title = metadata.title if metadata and metadata.title else self._extract_title_from_html(response.text)
        
        # è®¡ç®—è´¨é‡å¾—åˆ†
        quality_score = self._calculate_content_quality(content, len(response.text))
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_summary(content, max_length=200)
        
        # è®¡ç®—tokenæ•°é‡
        token_count = len(self.tokenizer.encode(content))
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=summary,
            metadata=metadata.__dict__ if metadata else {},
            extraction_method="trafilatura",
            token_count=token_count,
            quality_score=quality_score
        )
    
    async def _extract_with_newspaper(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨newspaper3kæå–"""
        article = Article(url, language='zh')
        await asyncio.to_thread(article.download)
        await asyncio.to_thread(article.parse)
        
        if not article.text:
            return None
        
        content = article.text
        quality_score = self._calculate_content_quality(content, len(article.html))
        summary = self._generate_summary(content, max_length=200)
        token_count = len(self.tokenizer.encode(content))
        
        return ExtractedContent(
            url=url,
            title=article.title or "æ— æ ‡é¢˜",
            content=content,
            summary=summary,
            metadata={
                "authors": article.authors,
                "publish_date": str(article.publish_date) if article.publish_date else None,
                "top_image": article.top_image
            },
            extraction_method="newspaper",
            token_count=token_count,
            quality_score=quality_score
        )
    
    async def _extract_with_readability(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨readabilityæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        doc = Document(response.text)
        title = doc.title()
        content_html = doc.summary()
        
        # è½¬æ¢ä¸ºçº¯æ–‡æœ¬
        soup = BeautifulSoup(content_html, 'html.parser')
        content = soup.get_text(separator='\n', strip=True)
        
        if not content:
            return None
        
        quality_score = self._calculate_content_quality(content, len(response.text))
        summary = self._generate_summary(content, max_length=200)
        token_count = len(self.tokenizer.encode(content))
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=summary,
            metadata={},
            extraction_method="readability",
            token_count=token_count,
            quality_score=quality_score
        )
    
    async def _extract_with_bs4(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨BeautifulSoupæå–ï¼ˆå…œåº•æ–¹æ³•ï¼‰"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for element in soup(["script", "style", "nav", "footer", "aside"]):
            element.decompose()
        
        # æå–æ ‡é¢˜
        title = self._extract_title_from_html(response.text)
        
        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        content_candidates = soup.find_all(['article', 'main', 'div'], 
                                         class_=re.compile(r'content|article|post|main', re.I))
        
        if content_candidates:
            content = content_candidates[0].get_text(separator='\n', strip=True)
        else:
            content = soup.get_text(separator='\n', strip=True)
        
        # æ¸…ç†å†…å®¹
        content = self._clean_text(content)
        
        if not content:
            return None
        
        quality_score = self._calculate_content_quality(content, len(response.text))
        summary = self._generate_summary(content, max_length=200)
        token_count = len(self.tokenizer.encode(content))
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=summary,
            metadata={},
            extraction_method="beautifulsoup",
            token_count=token_count,
            quality_score=quality_score
        )
    
    def _extract_title_from_html(self, html: str) -> str:
        """ä»HTMLä¸­æå–æ ‡é¢˜"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°è¯•å¤šç§æ ‡é¢˜æå–æ–¹æ³•
        title_candidates = [
            soup.find('title'),
            soup.find('h1'),
            soup.find('meta', property='og:title'),
            soup.find('meta', name='title')
        ]
        
        for candidate in title_candidates:
            if candidate:
                if candidate.name == 'meta':
                    title = candidate.get('content', '').strip()
                else:
                    title = candidate.get_text(strip=True)
                
                if title:
                    return title[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
        
        return ""
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æå–çš„æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # ç§»é™¤å¸¸è§çš„æ— ç”¨å†…å®¹
        patterns_to_remove = [
            r'Cookie.*?åŒæ„',
            r'è®¢é˜….*?newsletter',
            r'å…³æ³¨æˆ‘ä»¬.*?ç¤¾äº¤åª’ä½“',
            r'ç‰ˆæƒæ‰€æœ‰.*?ä¿ç•™',
            r'å¹¿å‘Š',
            r'Advertisement'
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _calculate_content_quality(self, content: str, html_length: int) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡å¾—åˆ†"""
        if not content:
            return 0.0
        
        # å†…å®¹é•¿åº¦å¾—åˆ†ï¼ˆé€‚ä¸­é•¿åº¦å¾—åˆ†é«˜ï¼‰
        length_score = min(len(content) / 1000, 1.0) if len(content) > 100 else 0.1
        
        # å†…å®¹å¯†åº¦å¾—åˆ†ï¼ˆæ–‡æœ¬å†…å®¹å HTMLçš„æ¯”ä¾‹ï¼‰
        density_score = len(content) / max(html_length, 1) if html_length > 0 else 0.5
        
        # ç»“æ„åŒ–å¾—åˆ†ï¼ˆåŒ…å«æ®µè½ã€å¥å­çš„ç»“æ„åŒ–ç¨‹åº¦ï¼‰
        structure_score = min(content.count('\n') / 10, 1.0) + min(content.count('.') / 20, 1.0)
        
        # ç»¼åˆå¾—åˆ†
        return (length_score * 0.4 + density_score * 0.3 + structure_score * 0.3)
    
    def _generate_summary(self, content: str, max_length: int = 200) -> str:
        """ç”Ÿæˆå†…å®¹æ‘˜è¦"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        summary = ""
        for sentence in sentences[:3]:  # å–å‰3å¥
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + "ã€‚"
            else:
                break
        
        return summary.strip()

# ===== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå†…å®¹é•¿åº¦æ§åˆ¶ä¸ä¼˜åŒ– =====

class ContentOptimizer:
    """å†…å®¹é•¿åº¦æ§åˆ¶ä¸ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_tokens: int = 4000, reserve_tokens: int = 1000):
        self.max_tokens = max_tokens
        self.reserve_tokens = reserve_tokens  # ä¸ºå¯¹è¯å†å²é¢„ç•™çš„token
        self.available_tokens = max_tokens - reserve_tokens
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def optimize_content_for_llm(self, extracted_contents: List[ExtractedContent], query: str) -> str:
        """ä¼˜åŒ–æå–çš„å†…å®¹ä»¥é€‚åº”LLMä¸Šä¸‹æ–‡é™åˆ¶"""
        
        if not extracted_contents:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        
        # 1. æŒ‰è´¨é‡æ’åº
        sorted_contents = sorted(extracted_contents, 
                                key=lambda x: x.quality_score, reverse=True)
        
        # 2. è®¡ç®—æŸ¥è¯¢çš„tokenæ¶ˆè€—
        query_tokens = len(self.tokenizer.encode(query))
        remaining_tokens = self.available_tokens - query_tokens - 200  # é¢„ç•™æ ¼å¼åŒ–token
        
        # 3. æ™ºèƒ½é€‰æ‹©å’Œæˆªå–å†…å®¹
        optimized_parts = []
        used_tokens = 0
        
        for i, content in enumerate(sorted_contents):
            if used_tokens >= remaining_tokens:
                break
            
            # ä¸ºæ¯ä¸ªæ¥æºåˆ†é…tokené¢„ç®—
            source_budget = min(
                remaining_tokens - used_tokens,
                remaining_tokens // max(len(sorted_contents) - i, 1)
            )
            
            # æå–å…³é”®éƒ¨åˆ†
            key_content = self._extract_key_content(content, query, source_budget)
            
            if key_content:
                content_tokens = len(self.tokenizer.encode(key_content))
                if used_tokens + content_tokens <= remaining_tokens:
                    optimized_parts.append({
                        'title': content.title,
                        'url': content.url,
                        'content': key_content,
                        'tokens': content_tokens
                    })
                    used_tokens += content_tokens
        
        # 4. æ ¼å¼åŒ–æœ€ç»ˆå†…å®¹
        return self._format_optimized_content(optimized_parts, query)
    
    def _extract_key_content(self, content: ExtractedContent, query: str, token_budget: int) -> str:
        """æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å†…å®¹ç‰‡æ®µ"""
        full_text = content.content
        query_terms = set(query.lower().split())
        
        # åˆ†æ®µ
        paragraphs = [p.strip() for p in full_text.split('\n') if len(p.strip()) > 50]
        
        if not paragraphs:
            return full_text[:token_budget * 3]  # ç²—ç•¥ä¼°ç®—å­—ç¬¦æ•°
        
        # è®¡ç®—æ¯æ®µçš„ç›¸å…³æ€§å¾—åˆ†
        scored_paragraphs = []
        for para in paragraphs:
            score = self._calculate_relevance_score(para, query_terms)
            tokens = len(self.tokenizer.encode(para))
            scored_paragraphs.append({
                'text': para,
                'score': score,
                'tokens': tokens
            })
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        scored_paragraphs.sort(key=lambda x: x['score'], reverse=True)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½
        selected_content = []
        used_tokens = 0
        
        for para in scored_paragraphs:
            if used_tokens + para['tokens'] <= token_budget:
                selected_content.append(para['text'])
                used_tokens += para['tokens']
            elif used_tokens < token_budget * 0.8:  # å¦‚æœè¿˜æœ‰è¾ƒå¤šé¢„ç®—ï¼Œå°è¯•æˆªå–
                remaining = token_budget - used_tokens
                truncated = self._truncate_to_tokens(para['text'], remaining)
                if truncated:
                    selected_content.append(truncated)
                break
        
        return '\n'.join(selected_content)
    
    def _calculate_relevance_score(self, text: str, query_terms: set) -> float:
        """è®¡ç®—æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§å¾—åˆ†"""
        text_lower = text.lower()
        text_words = set(text_lower.split())
        
        # è¯æ±‡åŒ¹é…å¾—åˆ†
        exact_matches = len(query_terms.intersection(text_words))
        partial_matches = sum(1 for term in query_terms 
                            if any(term in word for word in text_words))
        
        # ä½ç½®å¾—åˆ†ï¼ˆæŸ¥è¯¢è¯åœ¨æ–‡æœ¬å¼€å¤´çš„æƒé‡æ›´é«˜ï¼‰
        position_score = 0
        for term in query_terms:
            pos = text_lower.find(term)
            if pos >= 0:
                position_score += max(0, 1 - pos / len(text))
        
        # ç»¼åˆå¾—åˆ†
        vocab_score = (exact_matches * 2 + partial_matches) / len(query_terms)
        total_score = vocab_score * 0.7 + position_score * 0.3
        
        return total_score
    
    def _truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """æŒ‰tokenæ•°é‡æˆªå–æ–‡æœ¬"""
        tokens = self.tokenizer.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        truncated_tokens = tokens[:max_tokens]
        return self.tokenizer.decode(truncated_tokens) + "..."
    
    def _format_optimized_content(self, content_parts: List[Dict], query: str) -> str:
        """æ ¼å¼åŒ–ä¼˜åŒ–åçš„å†…å®¹"""
        if not content_parts:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„æœ‰æ•ˆå†…å®¹ã€‚"
        
        formatted_parts = [f"åŸºäºæœç´¢æŸ¥è¯¢ '{query}' æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n"]
        
        for i, part in enumerate(content_parts, 1):
            formatted_parts.append(f"""
ã€æ¥æº {i}ã€‘{part['title']}
é“¾æ¥ï¼š{part['url']}
å†…å®¹ï¼š{part['content']}
""")
        
        # æ·»åŠ tokenä½¿ç”¨ç»Ÿè®¡
        total_tokens = sum(part['tokens'] for part in content_parts)
        formatted_parts.append(f"\n[ä¿¡æ¯æ¥æºæ•°é‡: {len(content_parts)} | å†…å®¹tokens: {total_tokens}]")
        
        return "\n".join(formatted_parts)

# ===== ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´çš„æœç´¢å·¥å…·é›†æˆ ===

class IntelligentWebSearch:
    """æ™ºèƒ½ç½‘ç»œæœç´¢å·¥å…·"""
    
    def __init__(self, max_results: int = 5, max_content_tokens: int = 4000):
        self.search_engine = WebSearchEngine()
        self.content_extractor = WebContentExtractor()
        self.content_optimizer = ContentOptimizer(max_tokens=max_content_tokens)
        self.max_results = max_results
    
    async def search_and_extract(self, query: str, extract_content: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æœç´¢å’Œå†…å®¹æå–æµç¨‹"""
        
        try:
            # 1. æ‰§è¡Œæœç´¢
            search_results = await self.search_engine.search_duckduckgo(query, self.max_results)
            
            if not search_results:
                return {
                    "success": False,
                    "message": f"æ²¡æœ‰æ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ",
                    "results": []
                }
            
            result = {
                "success": True,
                "query": query,
                "search_results": [
                    {
                        "title": r.title,
                        "url": r.url,
                        "snippet": r.snippet,
                        "rank": r.rank
                    }
                    for r in search_results
                ],
                "extracted_contents": []
            }
            
            if not extract_content:
                return result
            
            # 2. æå–å†…å®¹
            urls_to_extract = [r.url for r in search_results[:3]]  # åªæå–å‰3ä¸ªç»“æœ
            extracted_contents = await self.content_extractor.extract_multiple_urls(urls_to_extract)
            
            result["extracted_contents"] = [
                {
                    "title": content.title,
                    "url": content.url,
                    "summary": content.summary,
                    "token_count": content.token_count,
                    "quality_score": content.quality_score,
                    "extraction_method": content.extraction_method
                }
                for content in extracted_contents
            ]
            
            # 3. ä¼˜åŒ–å†…å®¹
            if extracted_contents:
                optimized_content = self.content_optimizer.optimize_content_for_llm(
                    extracted_contents, query
                )
                result["optimized_content"] = optimized_content
            else:
                result["optimized_content"] = "æ— æ³•æå–ç½‘é¡µå†…å®¹ï¼Œä»…æä¾›æœç´¢ç»“æœæ‘˜è¦ã€‚"
            
            return result
        
        except Exception as e:
            logging.error(f"æœç´¢å’Œæå–å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "results": []
            }
    
    async def close(self):
        """å…³é—­èµ„æº"""
        await self.search_engine.http_client.aclose()
        await self.content_extractor.http_client.aclose()

# ===== ä½¿ç”¨ç¤ºä¾‹ ===

async def demo_intelligent_search():
    """æ™ºèƒ½æœç´¢æ¼”ç¤º"""
    
    search_tool = IntelligentWebSearch(max_results=5, max_content_tokens=3000)
    
    try:
        # æµ‹è¯•æŸ¥è¯¢
        query = "FGOå¥¥åšé¾™é…é˜Ÿ"
        
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
        result = await search_tool.search_and_extract(query, extract_content=True)
        
        if result["success"]:
            print(f"\nâœ… æœç´¢æˆåŠŸ!")
            print(f"ğŸ“Š æ‰¾åˆ° {len(result['search_results'])} ä¸ªæœç´¢ç»“æœ")
            print(f"ğŸ“„ æå–äº† {len(result['extracted_contents'])} ä¸ªç½‘é¡µå†…å®¹")
            
            print(f"\nğŸ“ ä¼˜åŒ–åçš„å†…å®¹ (ç”¨äºLLM):")
            print("=" * 80)
            print(result["optimized_content"])
            print("=" * 80)
        else:
            print(f"âŒ æœç´¢å¤±è´¥: {result['message']}")
    
    finally:
        await search_tool.close()

# ===== ä¸LangGraphé›†æˆçš„èŠ‚ç‚¹ç¤ºä¾‹ ===

async def create_web_search_node(search_tool: IntelligentWebSearch):
    """åˆ›å»ºç½‘ç»œæœç´¢èŠ‚ç‚¹"""
    
    async def web_search_node(state: Dict[str, Any]) -> Dict[str, Any]:
        """ç½‘ç»œæœç´¢èŠ‚ç‚¹å®ç°"""
        
        query = state.get("tool_input", {}).get("query", "")
        
        if not query:
            error_message = "æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"
            return {
                "messages": state["messages"] + [{"role": "assistant", "content": error_message}],
                "tool_result": {"success": False, "error": error_message}
            }
        
        # æ‰§è¡Œæœç´¢å’Œå†…å®¹æå–
        search_result = await search_tool.search_and_extract(query, extract_content=True)
        
        if search_result["success"]:
            # ä½¿ç”¨ä¼˜åŒ–åçš„å†…å®¹
            response_content = search_result["optimized_content"]
            
            return {
                "messages": state["messages"] + [{"role": "assistant", "content": response_content}],
                "tool_result": {
                    "success": True,
                    "answer": response_content,
                    "type": "web_search",
                    "sources": len(search_result["search_results"])
                }
            }
        else:
            error_message = search_result["message"]
            return {
                "messages": state["messages"] + [{"role": "assistant", "content": error_message}],
                "tool_result": {"success": False, "error": error_message}
            }
    
    return web_search_node

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_intelligent_search())