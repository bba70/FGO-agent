# === requirements.txt ===
"""
mcp>=1.0.0
httpx>=0.27.0
beautifulsoup4>=4.12.0
readability-lxml>=0.8.1
tiktoken>=0.5.0
trafilatura>=1.6.0
newspaper3k>=0.2.8
python-dotenv>=1.0.0
"""

# === .env é…ç½®æ–‡ä»¶ ===
"""
# æœç´¢é…ç½®
MAX_SEARCH_RESULTS=5
MAX_CONTENT_TOKENS=4000
TIMEOUT_SECONDS=20
DEBUG=false

# å¯é€‰ï¼šGoogle Search API
GOOGLE_API_KEY=your_google_api_key
GOOGLE_CX=your_google_cx
"""

# === web_search_mcp.py ===

import asyncio
import os
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass

import httpx
from bs4 import BeautifulSoup
from readability import Document
import tiktoken
import trafilatura
from newspaper import Article
from dotenv import load_dotenv

from mcp.server.models import InitializationOptions
from mcp.server import Server
from mcp.types import Tool, TextContent

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("web-search-mcp")

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    url: str
    snippet: str
    source: str
    rank: int = 0

@dataclass
class ExtractedContent:
    """æå–çš„ç½‘é¡µå†…å®¹"""
    url: str
    title: str
    content: str
    summary: str
    extraction_method: str
    token_count: int
    quality_score: float

class WebSearchMCP:
    """ç½‘ç»œæœç´¢MCPå·¥å…·æ ¸å¿ƒç±»"""
    
    def __init__(self):
        self.max_results = int(os.getenv("MAX_SEARCH_RESULTS", "5"))
        self.max_tokens = int(os.getenv("MAX_CONTENT_TOKENS", "4000"))
        self.timeout = int(os.getenv("TIMEOUT_SECONDS", "20"))
        self.debug = os.getenv("DEBUG", "false").lower() == "true"
        
        # Google APIé…ç½®ï¼ˆå¯é€‰ï¼‰
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cx = os.getenv("GOOGLE_CX")
        
        # HTTPå®¢æˆ·ç«¯
        self.http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(self.timeout),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate'
            },
            follow_redirects=True
        )
        
        # Tokenè®¡ç®—å™¨
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        if self.debug:
            logger.setLevel(logging.DEBUG)
    
    async def search_web(self, query: str, max_results: Optional[int] = None) -> List[SearchResult]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        if max_results is None:
            max_results = self.max_results
        
        logger.info(f"å¼€å§‹æœç´¢: {query}")
        
        try:
            # ä¼˜å…ˆä½¿ç”¨Google APIï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.google_api_key and self.google_cx:
                results = await self._search_with_google(query, max_results)
                if results:
                    return results
            
            # ä½¿ç”¨DuckDuckGoä½œä¸ºåå¤‡
            return await self._search_with_duckduckgo(query, max_results)
        
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_with_google(self, query: str, max_results: int) -> List[SearchResult]:
        """ä½¿ç”¨Google Custom Search API"""
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': query,
                'num': min(max_results, 10),
                'hl': 'zh-CN',
                'gl': 'cn'
            }
            
            response = await self.http_client.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for i, item in enumerate(data.get('items', [])):
                results.append(SearchResult(
                    title=item.get('title', ''),
                    url=item.get('link', ''),
                    snippet=item.get('snippet', ''),
                    source="google_api",
                    rank=i + 1
                ))
            
            logger.info(f"Googleæœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
        
        except Exception as e:
            logger.warning(f"Googleæœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_with_duckduckgo(self, query: str, max_results: int) -> List[SearchResult]:
        """ä½¿ç”¨DuckDuckGoæœç´¢"""
        try:
            search_url = "https://html.duckduckgo.com/html/"
            params = {
                'q': query,
                'b': '',
                'kl': 'cn-zh',
                's': '0'
            }
            
            response = await self.http_client.post(search_url, data=params)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            result_containers = soup.find_all('div', class_='result')
            
            for i, container in enumerate(result_containers[:max_results]):
                try:
                    title_link = container.find('a', class_='result__a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    url = title_link.get('href', '')
                    
                    snippet_elem = container.find('a', class_='result__snippet')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    if title and url:
                        results.append(SearchResult(
                            title=title,
                            url=url,
                            snippet=snippet,
                            source="duckduckgo",
                            rank=i + 1
                        ))
                
                except Exception as e:
                    logger.debug(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
                    continue
            
            logger.info(f"DuckDuckGoæœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
        
        except Exception as e:
            logger.error(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")
            return []
    
    async def extract_content(self, url: str) -> Optional[ExtractedContent]:
        """æå–ç½‘é¡µå†…å®¹"""
        logger.debug(f"å¼€å§‹æå–å†…å®¹: {url}")
        
        extraction_methods = [
            ("trafilatura", self._extract_with_trafilatura),
            ("newspaper", self._extract_with_newspaper),
            ("readability", self._extract_with_readability),
            ("beautifulsoup", self._extract_with_bs4)
        ]
        
        for method_name, method_func in extraction_methods:
            try:
                content = await method_func(url)
                if content and content.quality_score > 0.3:
                    logger.debug(f"ä½¿ç”¨ {method_name} æˆåŠŸæå–å†…å®¹")
                    return content
            except Exception as e:
                logger.debug(f"{method_name} æå–å¤±è´¥: {e}")
                continue
        
        logger.warning(f"æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥: {url}")
        return None
    
    async def _extract_with_trafilatura(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨trafilaturaæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        content = trafilatura.extract(
            response.text,
            include_comments=False,
            include_tables=True,
            include_formatting=False
        )
        
        if not content:
            return None
        
        metadata = trafilatura.extract_metadata(response.text)
        title = metadata.title if metadata and metadata.title else self._extract_title(response.text)
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=self._generate_summary(content),
            extraction_method="trafilatura",
            token_count=len(self.tokenizer.encode(content)),
            quality_score=self._calculate_quality(content, len(response.text))
        )
    
    async def _extract_with_newspaper(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨newspaper3kæå–"""
        article = Article(url, language='zh')
        await asyncio.to_thread(article.download)
        await asyncio.to_thread(article.parse)
        
        if not article.text:
            return None
        
        return ExtractedContent(
            url=url,
            title=article.title or "æ— æ ‡é¢˜",
            content=article.text,
            summary=self._generate_summary(article.text),
            extraction_method="newspaper",
            token_count=len(self.tokenizer.encode(article.text)),
            quality_score=self._calculate_quality(article.text, len(article.html or ""))
        )
    
    async def _extract_with_readability(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨readabilityæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        doc = Document(response.text)
        content_html = doc.summary()
        
        soup = BeautifulSoup(content_html, 'html.parser')
        content = soup.get_text(separator='\n', strip=True)
        
        if not content:
            return None
        
        return ExtractedContent(
            url=url,
            title=doc.title() or "æ— æ ‡é¢˜",
            content=content,
            summary=self._generate_summary(content),
            extraction_method="readability",
            token_count=len(self.tokenizer.encode(content)),
            quality_score=self._calculate_quality(content, len(response.text))
        )
    
    async def _extract_with_bs4(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨BeautifulSoupæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤æ— ç”¨å…ƒç´ 
        for element in soup(["script", "style", "nav", "footer", "aside"]):
            element.decompose()
        
        title = self._extract_title(response.text)
        
        # æå–ä¸»è¦å†…å®¹
        content_candidates = soup.find_all(['article', 'main', 'div'], 
                                         class_=re.compile(r'content|article|post|main', re.I))
        
        if content_candidates:
            content = content_candidates[0].get_text(separator='\n', strip=True)
        else:
            content = soup.get_text(separator='\n', strip=True)
        
        content = self._clean_text(content)
        
        if not content:
            return None
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=self._generate_summary(content),
            extraction_method="beautifulsoup",
            token_count=len(self.tokenizer.encode(content)),
            quality_score=self._calculate_quality(content, len(response.text))
        )
    
    def _extract_title(self, html: str) -> str:
        """æå–æ ‡é¢˜"""
        soup = BeautifulSoup(html, 'html.parser')
        
        for selector in ['title', 'h1', 'meta[property="og:title"]']:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    title = element.get('content', '').strip()
                else:
                    title = element.get_text(strip=True)
                if title:
                    return title[:100]
        return ""
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        return text.strip()
    
    def _generate_summary(self, content: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        summary = ""
        for sentence in sentences[:3]:
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + "ã€‚"
            else:
                break
        
        return summary.strip()
    
    def _calculate_quality(self, content: str, html_length: int) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡"""
        if not content:
            return 0.0
        
        length_score = min(len(content) / 1000, 1.0) if len(content) > 100 else 0.1
        density_score = len(content) / max(html_length, 1) if html_length > 0 else 0.5
        
        return (length_score * 0.6 + density_score * 0.4)
    
    def _optimize_content_for_llm(self, extracted_contents: List[ExtractedContent], query: str) -> str:
        """ä¼˜åŒ–å†…å®¹é•¿åº¦ä»¥é€‚åº”LLM"""
        if not extracted_contents:
            return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³å†…å®¹ã€‚"
        
        # æŒ‰è´¨é‡æ’åº
        sorted_contents = sorted(extracted_contents, key=lambda x: x.quality_score, reverse=True)
        
        # è®¡ç®—å¯ç”¨token
        query_tokens = len(self.tokenizer.encode(query))
        available_tokens = self.max_tokens - query_tokens - 300  # é¢„ç•™æ ¼å¼åŒ–token
        
        # æ„å»ºä¼˜åŒ–å†…å®¹
        result_parts = [f"åŸºäºæœç´¢ '{query}' æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š\n"]
        used_tokens = len(self.tokenizer.encode(result_parts[0]))
        
        for i, content in enumerate(sorted_contents):
            if used_tokens >= available_tokens:
                break
            
            # ä¸ºæ¯ä¸ªæ¥æºåˆ†é…tokené¢„ç®—
            remaining_tokens = available_tokens - used_tokens
            source_budget = min(remaining_tokens, remaining_tokens // (len(sorted_contents) - i))
            
            # æˆªå–å†…å®¹
            content_text = content.content
            content_tokens = len(self.tokenizer.encode(content_text))
            
            if content_tokens > source_budget:
                # æˆªå–åˆ°åˆé€‚é•¿åº¦
                tokens = self.tokenizer.encode(content_text)
                truncated_tokens = tokens[:source_budget - 10]  # é¢„ç•™"..."çš„token
                content_text = self.tokenizer.decode(truncated_tokens) + "..."
                
                source_info = f"""
                            ã€æ¥æº {i+1}ã€‘{content.title}
                            é“¾æ¥ï¼š{content.url}
                            å†…å®¹ï¼š{content_text}
                            """
            
            source_tokens = len(self.tokenizer.encode(source_info))
            if used_tokens + source_tokens <= available_tokens:
                result_parts.append(source_info)
                used_tokens += source_tokens
        
        final_content = "\n".join(result_parts)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        final_content += f"\n[ä½¿ç”¨äº† {len([p for p in result_parts if p.startswith('ã€æ¥æº')])} ä¸ªä¿¡æ¯æº]"
        
        return final_content
    
    async def close(self):
        """å…³é—­èµ„æº"""
        await self.http_client.aclose()

# === MCPæœåŠ¡å™¨å®ç° ===

# åˆ›å»ºå…¨å±€å·¥å…·å®ä¾‹
web_search_tool = WebSearchMCP()

# åˆ›å»ºMCPæœåŠ¡å™¨
server = Server("web-search-mcp")

@server.list_tools()
async def handle_list_tools() -> List[Tool]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·"""
    return [
        Tool(
            name="search_web",
            description="åœ¨ç½‘ç»œä¸Šæœç´¢ä¿¡æ¯å¹¶è¿”å›æœç´¢ç»“æœ",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢è¯",
                        "minLength": 1,
                        "maxLength": 200
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§æœç´¢ç»“æœæ•°é‡",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10
                    }
                },
                "required": ["query"],
                "additionalProperties": False
            }
        ),
        Tool(
            name="extract_webpage",
            description="æå–æŒ‡å®šç½‘é¡µçš„è¯¦ç»†å†…å®¹",
            inputSchema={
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "è¦æå–å†…å®¹çš„ç½‘é¡µURL",
                        "format": "uri"
                    }
                },
                "required": ["url"],
                "additionalProperties": False
            }
        ),
        Tool(
            name="search_and_extract",
            description="æœç´¢å¹¶è‡ªåŠ¨æå–ç½‘é¡µå†…å®¹ï¼Œè¿”å›ä¼˜åŒ–åçš„ä¿¡æ¯",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢è¯",
                        "minLength": 1,
                        "maxLength": 200
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§æœç´¢ç»“æœæ•°é‡",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 8
                    },
                    "extract_count": {
                        "type": "integer",
                        "description": "æå–è¯¦ç»†å†…å®¹çš„ç½‘é¡µæ•°é‡",
                        "default": 3,
                        "minimum": 1,
                        "maximum": 5
                    }
                },
                "required": ["query"],
                "additionalProperties": False
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""
    try:
        if name == "search_web":
            query = arguments.get("query", "").strip()
            max_results = arguments.get("max_results", 5)
            
            if not query:
                return [TextContent(type="text", text="é”™è¯¯: æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")]
            
            search_results = await web_search_tool.search_web(query, max_results)
            
            if not search_results:
                return [TextContent(type="text", text=f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ")]
            
            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            formatted_results = [f"ğŸ” æœç´¢æŸ¥è¯¢: {query}\n"]
            for result in search_results:
                formatted_results.append(
                    f"{result.rank}. **{result.title}**\n"
                    f"   ğŸ”— {result.url}\n"
                    f"   ğŸ“„ {result.snippet}\n"
                    f"   ğŸ“Š æ¥æº: {result.source}\n"
                )
            
            return [TextContent(type="text", text="\n".join(formatted_results))]
        
        elif name == "extract_webpage":
            url = arguments.get("url", "").strip()
            
            if not url:
                return [TextContent(type="text", text="é”™è¯¯: URLä¸èƒ½ä¸ºç©º")]
            
            extracted_content = await web_search_tool.extract_content(url)
            
            if not extracted_content:
                return [TextContent(type="text", text=f"æ— æ³•æå–ç½‘é¡µå†…å®¹: {url}")]
            
            response = f"""ğŸ“„ ç½‘é¡µå†…å®¹æå–ç»“æœ

                            ğŸ”— URL: {extracted_content.url}
                            ğŸ“ æ ‡é¢˜: {extracted_content.title}
                            ğŸ”§ æå–æ–¹æ³•: {extracted_content.extraction_method}
                            ğŸ“Š è´¨é‡è¯„åˆ†: {extracted_content.quality_score:.2f}
                            ğŸ¯ Tokenæ•°é‡: {extracted_content.token_count}

                            ğŸ“‹ å†…å®¹æ‘˜è¦:
                            {extracted_content.summary}

                            ğŸ“– å®Œæ•´å†…å®¹:
                            {extracted_content.content}
                            """
            
            return [TextContent(type="text", text=response)]
        
        elif name == "search_and_extract":
            query = arguments.get("query", "").strip()
            max_results = arguments.get("max_results", 5)
            extract_count = arguments.get("extract_count", 3)
            
            if not query:
                return [TextContent(type="text", text="é”™è¯¯: æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")]
            
            # æ‰§è¡Œæœç´¢
            search_results = await web_search_tool.search_web(query, max_results)
            
            if not search_results:
                return [TextContent(type="text", text=f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ")]
            
            # æå–å†…å®¹
            urls_to_extract = [result.url for result in search_results[:extract_count]]
            extracted_contents = []
            
            for url in urls_to_extract:
                try:
                    content = await web_search_tool.extract_content(url)
                    if content:
                        extracted_contents.append(content)
                except Exception as e:
                    logger.warning(f"æå–å†…å®¹å¤±è´¥ {url}: {e}")
            
            if not extracted_contents:
                # å¦‚æœæ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œè¿”å›æœç´¢ç»“æœ
                formatted_results = [f"ğŸ” æœç´¢ç»“æœ (æœªèƒ½æå–è¯¦ç»†å†…å®¹): {query}\n"]
                for result in search_results:
                    formatted_results.append(
                        f"{result.rank}. {result.title}\n   {result.snippet}\n   {result.url}\n"
                    )
                return [TextContent(type="text", text="\n".join(formatted_results))]
            
            # ä¼˜åŒ–å†…å®¹
            optimized_content = web_search_tool._optimize_content_for_llm(extracted_contents, query)
            
            return [TextContent(type="text", text=optimized_content)]
        
        else:
            return [TextContent(type="text", text=f"é”™è¯¯: æœªçŸ¥çš„å·¥å…·åç§° '{name}'")]
    
    except Exception as e:
        logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
        return [TextContent(type="text", text=f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}")]

async def main():
    """MCPæœåŠ¡å™¨ä¸»å‡½æ•°"""
    try:
        logger.info("Web Search MCP Server å¯åŠ¨ä¸­...")
        
        import mcp.server.stdio
        
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="web-search-mcp",
                    server_version="1.0.0"
                )
            )
    
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨è¿è¡Œé”™è¯¯: {e}")
    finally:
        await web_search_tool.close()

if __name__ == "__main__":
    asyncio.run(main())