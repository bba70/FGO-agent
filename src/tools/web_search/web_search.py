# === web_search_fastmcp.py ===

import asyncio
import os
import logging
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from urllib.parse import urlparse

import httpx
from bs4 import BeautifulSoup
from readability import Document
import tiktoken
import trafilatura
from newspaper import Article
from dotenv import load_dotenv

from fastmcp import FastMCP

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("web-search-mcp")


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    url: str
    snippet: str
    source: str
    rank: int = 0

@dataclass
class ExtractedContent:
    """æå–çš„ç½‘é¡µå†…å®¹"""
    url: str
    title: str
    content: str
    summary: str
    extraction_method: str
    token_count: int
    quality_score: float



class WebSearchTool:
    """ç½‘ç»œæœç´¢å·¥å…·æ ¸å¿ƒç±»"""
    
    def __init__(self):
        self.max_results = int(os.getenv("MAX_SEARCH_RESULTS", "5"))
        self.max_tokens = int(os.getenv("MAX_CONTENT_TOKENS", "4000"))
        self.timeout = int(os.getenv("TIMEOUT_SECONDS", "20"))
        self.debug = os.getenv("DEBUG", "false").lower() == "true"
        
        # Google APIé…ç½®ï¼ˆå¯é€‰ï¼‰
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cx = os.getenv("GOOGLE_CX")
        
        # HTTPå®¢æˆ·ç«¯
        self.http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(self.timeout),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate'
            },
            follow_redirects=True
        )
        
        # Tokenè®¡ç®—å™¨
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        if self.debug:
            logger.setLevel(logging.DEBUG)
    
    async def search_web(self, query: str, max_results: Optional[int] = None) -> List[SearchResult]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        if max_results is None:
            max_results = self.max_results
        
        logger.info(f"å¼€å§‹æœç´¢: {query}")
        
        try:
            # ä¼˜å…ˆä½¿ç”¨Google APIï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.google_api_key and self.google_cx:
                results = await self._search_with_google(query, max_results)
                if results:
                    return results
            
            # ä½¿ç”¨DuckDuckGoä½œä¸ºåå¤‡
            return await self._search_with_duckduckgo(query, max_results)
        
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_with_google(self, query: str, max_results: int) -> List[SearchResult]:
        """ä½¿ç”¨Google Custom Search API"""
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': query,
                'num': min(max_results, 10),
                'hl': 'zh-CN',
                'gl': 'cn'
            }
            
            response = await self.http_client.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for i, item in enumerate(data.get('items', [])):
                results.append(SearchResult(
                    title=item.get('title', ''),
                    url=item.get('link', ''),
                    snippet=item.get('snippet', ''),
                    source="google_api",
                    rank=i + 1
                ))
            
            logger.info(f"Googleæœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
        
        except Exception as e:
            logger.warning(f"Googleæœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_with_duckduckgo(self, query: str, max_results: int) -> List[SearchResult]:
        """ä½¿ç”¨DuckDuckGoæœç´¢"""
        try:
            search_url = "https://html.duckduckgo.com/html/"
            params = {
                'q': query,
                'b': '',
                'kl': 'cn-zh',
                's': '0'
            }
            
            response = await self.http_client.post(search_url, data=params)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            result_containers = soup.find_all('div', class_='result')
            
            for i, container in enumerate(result_containers[:max_results]):
                try:
                    title_link = container.find('a', class_='result__a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    url = title_link.get('href', '')
                    
                    snippet_elem = container.find('a', class_='result__snippet')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    if title and url:
                        results.append(SearchResult(
                            title=title,
                            url=url,
                            snippet=snippet,
                            source="duckduckgo",
                            rank=i + 1
                        ))
                
                except Exception as e:
                    logger.debug(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
                    continue
            
            logger.info(f"DuckDuckGoæœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
        
        except Exception as e:
            logger.error(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")
            return []
    
    async def extract_content(self, url: str) -> Optional[ExtractedContent]:
        """æå–ç½‘é¡µå†…å®¹"""
        logger.debug(f"å¼€å§‹æå–å†…å®¹: {url}")
        
        extraction_methods = [
            ("trafilatura", self._extract_with_trafilatura),
            ("newspaper", self._extract_with_newspaper),
            ("readability", self._extract_with_readability),
            ("beautifulsoup", self._extract_with_bs4)
        ]
        
        for method_name, method_func in extraction_methods:
            try:
                content = await method_func(url)
                if content and content.quality_score > 0.3:
                    logger.debug(f"ä½¿ç”¨ {method_name} æˆåŠŸæå–å†…å®¹")
                    return content
            except Exception as e:
                logger.debug(f"{method_name} æå–å¤±è´¥: {e}")
                continue
        
        logger.warning(f"æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥: {url}")
        return None
    
    async def _extract_with_trafilatura(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨trafilaturaæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        content = trafilatura.extract(
            response.text,
            include_comments=False,
            include_tables=True,
            include_formatting=False
        )
        
        if not content:
            return None
        
        metadata = trafilatura.extract_metadata(response.text)
        title = metadata.title if metadata and metadata.title else self._extract_title(response.text)
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=self._generate_summary(content),
            extraction_method="trafilatura",
            token_count=len(self.tokenizer.encode(content)),
            quality_score=self._calculate_quality(content, len(response.text))
        )
    
    async def _extract_with_newspaper(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨newspaper3kæå–"""
        article = Article(url, language='zh')
        await asyncio.to_thread(article.download)
        await asyncio.to_thread(article.parse)
        
        if not article.text:
            return None
        
        return ExtractedContent(
            url=url,
            title=article.title or "æ— æ ‡é¢˜",
            content=article.text,
            summary=self._generate_summary(article.text),
            extraction_method="newspaper",
            token_count=len(self.tokenizer.encode(article.text)),
            quality_score=self._calculate_quality(article.text, len(article.html or ""))
        )
    
    async def _extract_with_readability(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨readabilityæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        doc = Document(response.text)
        content_html = doc.summary()
        
        soup = BeautifulSoup(content_html, 'html.parser')
        content = soup.get_text(separator='\n', strip=True)
        
        if not content:
            return None
        
        return ExtractedContent(
            url=url,
            title=doc.title() or "æ— æ ‡é¢˜",
            content=content,
            summary=self._generate_summary(content),
            extraction_method="readability",
            token_count=len(self.tokenizer.encode(content)),
            quality_score=self._calculate_quality(content, len(response.text))
        )
    
    async def _extract_with_bs4(self, url: str) -> Optional[ExtractedContent]:
        """ä½¿ç”¨BeautifulSoupæå–"""
        response = await self.http_client.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤æ— ç”¨å…ƒç´ 
        for element in soup(["script", "style", "nav", "footer", "aside"]):
            element.decompose()
        
        title = self._extract_title(response.text)
        
        # æå–ä¸»è¦å†…å®¹
        content_candidates = soup.find_all(['article', 'main', 'div'], 
                                         class_=re.compile(r'content|article|post|main', re.I))
        
        if content_candidates:
            content = content_candidates[0].get_text(separator='\n', strip=True)
        else:
            content = soup.get_text(separator='\n', strip=True)
        
        content = self._clean_text(content)
        
        if not content:
            return None
        
        return ExtractedContent(
            url=url,
            title=title or "æ— æ ‡é¢˜",
            content=content,
            summary=self._generate_summary(content),
            extraction_method="beautifulsoup",
            token_count=len(self.tokenizer.encode(content)),
            quality_score=self._calculate_quality(content, len(response.text))
        )
    
    def _extract_title(self, html: str) -> str:
        """æå–æ ‡é¢˜"""
        soup = BeautifulSoup(html, 'html.parser')
        
        for selector in ['title', 'h1', 'meta[property="og:title"]']:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    title = element.get('content', '').strip()
                else:
                    title = element.get_text(strip=True)
                if title:
                    return title[:100]
        return ""
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        return text.strip()
    
    def _generate_summary(self, content: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        summary = ""
        for sentence in sentences[:3]:
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + "ã€‚"
            else:
                break
        
        return summary.strip()
    
    def _calculate_quality(self, content: str, html_length: int) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡"""
        if not content:
            return 0.0
        
        length_score = min(len(content) / 1000, 1.0) if len(content) > 100 else 0.1
        density_score = len(content) / max(html_length, 1) if html_length > 0 else 0.5
        
        return (length_score * 0.6 + density_score * 0.4)
    
    def optimize_content_for_llm(self, extracted_contents: List[ExtractedContent], query: str) -> str:
        """ä¼˜åŒ–å†…å®¹é•¿åº¦ä»¥é€‚åº”LLM"""
        if not extracted_contents:
            return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³å†…å®¹ã€‚"
        
        # æŒ‰è´¨é‡æ’åº
        sorted_contents = sorted(extracted_contents, key=lambda x: x.quality_score, reverse=True)
        
        # è®¡ç®—å¯ç”¨token
        query_tokens = len(self.tokenizer.encode(query))
        available_tokens = self.max_tokens - query_tokens - 300  # é¢„ç•™æ ¼å¼åŒ–token
        
        # æ„å»ºä¼˜åŒ–å†…å®¹
        result_parts = [f"åŸºäºæœç´¢ '{query}' æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š\n"]
        used_tokens = len(self.tokenizer.encode(result_parts[0]))
        
        for i, content in enumerate(sorted_contents):
            if used_tokens >= available_tokens:
                break
            
            # ä¸ºæ¯ä¸ªæ¥æºåˆ†é…tokené¢„ç®—
            remaining_tokens = available_tokens - used_tokens
            source_budget = min(remaining_tokens, remaining_tokens // (len(sorted_contents) - i))
            
            # æˆªå–å†…å®¹
            content_text = content.content
            content_tokens = len(self.tokenizer.encode(content_text))
            
            if content_tokens > source_budget:
                # æˆªå–åˆ°åˆé€‚é•¿åº¦
                tokens = self.tokenizer.encode(content_text)
                truncated_tokens = tokens[:source_budget - 10]  # é¢„ç•™"..."çš„token
                content_text = self.tokenizer.decode(truncated_tokens) + "..."
            
            source_info = f"""
ã€æ¥æº {i+1}ã€‘{content.title}
é“¾æ¥ï¼š{content.url}
å†…å®¹ï¼š{content_text}
"""
            
            source_tokens = len(self.tokenizer.encode(source_info))
            if used_tokens + source_tokens <= available_tokens:
                result_parts.append(source_info)
                used_tokens += source_tokens
        
        final_content = "\n".join(result_parts)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        final_content += f"\n[ä½¿ç”¨äº† {len([p for p in result_parts if p.startswith('ã€æ¥æº')])} ä¸ªä¿¡æ¯æº]"
        
        return final_content
    
    async def close(self):
        """å…³é—­èµ„æº"""
        await self.http_client.aclose()




# åˆ›å»ºå…¨å±€å·¥å…·å®ä¾‹
web_search_tool = WebSearchTool()

mcp = FastMCP("web-search-mcp")


@mcp.tool()
async def search_web(query: str, max_results: int = 5) -> str:
    """
    åœ¨ç½‘ç»œä¸Šæœç´¢ä¿¡æ¯å¹¶è¿”å›æœç´¢ç»“æœ
    
    Args:
        query: æœç´¢æŸ¥è¯¢è¯ï¼ˆå¿…å¡«ï¼‰
        max_results: æœ€å¤§æœç´¢ç»“æœæ•°é‡ï¼Œé»˜è®¤5ï¼ŒèŒƒå›´1-10
    
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
    """
    if not query or not query.strip():
        return "é”™è¯¯: æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"
    
    if not (1 <= max_results <= 10):
        max_results = 5
    
    search_results = await web_search_tool.search_web(query.strip(), max_results)
    
    if not search_results:
        return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ"
    
    # æ ¼å¼åŒ–æœç´¢ç»“æœ
    formatted_results = [f"ğŸ” æœç´¢æŸ¥è¯¢: {query}\n"]
    for result in search_results:
        formatted_results.append(
            f"{result.rank}. **{result.title}**\n"
            f"   ğŸ”— {result.url}\n"
            f"   ğŸ“„ {result.snippet}\n"
            f"   ğŸ“Š æ¥æº: {result.source}\n"
        )
    
    return "\n".join(formatted_results)


@mcp.tool()
async def extract_webpage(url: str) -> str:
    """
    æå–æŒ‡å®šç½‘é¡µçš„è¯¦ç»†å†…å®¹
    
    Args:
        url: è¦æå–å†…å®¹çš„ç½‘é¡µURLï¼ˆå¿…å¡«ï¼‰
    
    Returns:
        æå–çš„ç½‘é¡µå†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ‘˜è¦å’Œå®Œæ•´å†…å®¹
    """
    if not url or not url.strip():
        return "é”™è¯¯: URLä¸èƒ½ä¸ºç©º"
    
    extracted_content = await web_search_tool.extract_content(url.strip())
    
    if not extracted_content:
        return f"æ— æ³•æå–ç½‘é¡µå†…å®¹: {url}"
    
    response = f"""ğŸ“„ ç½‘é¡µå†…å®¹æå–ç»“æœ

ğŸ”— URL: {extracted_content.url}
ğŸ“ æ ‡é¢˜: {extracted_content.title}
ğŸ”§ æå–æ–¹æ³•: {extracted_content.extraction_method}
ğŸ“Š è´¨é‡è¯„åˆ†: {extracted_content.quality_score:.2f}
ğŸ¯ Tokenæ•°é‡: {extracted_content.token_count}

ğŸ“‹ å†…å®¹æ‘˜è¦:
{extracted_content.summary}

ğŸ“– å®Œæ•´å†…å®¹:
{extracted_content.content}
"""
    
    return response


@mcp.tool()
async def search_and_extract(
    query: str, 
    max_results: int = 5, 
    extract_count: int = 3
) -> str:
    """
    æœç´¢å¹¶è‡ªåŠ¨æå–ç½‘é¡µå†…å®¹ï¼Œè¿”å›ä¼˜åŒ–åçš„ä¿¡æ¯ï¼ˆæ¨èä½¿ç”¨ï¼‰
    
    Args:
        query: æœç´¢æŸ¥è¯¢è¯ï¼ˆå¿…å¡«ï¼‰
        max_results: æœ€å¤§æœç´¢ç»“æœæ•°é‡ï¼Œé»˜è®¤5ï¼ŒèŒƒå›´1-8
        extract_count: æå–è¯¦ç»†å†…å®¹çš„ç½‘é¡µæ•°é‡ï¼Œé»˜è®¤3ï¼ŒèŒƒå›´1-5
    
    Returns:
        ä¼˜åŒ–åçš„ç»¼åˆä¿¡æ¯ï¼Œé€‚åˆLLMå¤„ç†
    """
    if not query or not query.strip():
        return "é”™è¯¯: æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"
    
    # å‚æ•°éªŒè¯
    max_results = max(1, min(max_results, 8))
    extract_count = max(1, min(extract_count, 5))
    
    # æ‰§è¡Œæœç´¢
    search_results = await web_search_tool.search_web(query.strip(), max_results)
    
    if not search_results:
        return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ"
    
    # æå–å†…å®¹
    urls_to_extract = [result.url for result in search_results[:extract_count]]
    extracted_contents = []
    
    for url in urls_to_extract:
        try:
            content = await web_search_tool.extract_content(url)
            if content:
                extracted_contents.append(content)
        except Exception as e:
            logger.warning(f"æå–å†…å®¹å¤±è´¥ {url}: {e}")
    
    if not extracted_contents:
        # å¦‚æœæ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œè¿”å›æœç´¢ç»“æœ
        formatted_results = [f"ğŸ” æœç´¢ç»“æœ (æœªèƒ½æå–è¯¦ç»†å†…å®¹): {query}\n"]
        for result in search_results:
            formatted_results.append(
                f"{result.rank}. {result.title}\n   {result.snippet}\n   {result.url}\n"
            )
        return "\n".join(formatted_results)
    
    # ä¼˜åŒ–å†…å®¹
    optimized_content = web_search_tool.optimize_content_for_llm(extracted_contents, query)
    
    return optimized_content



if __name__ == "__main__":
    mcp.run()
